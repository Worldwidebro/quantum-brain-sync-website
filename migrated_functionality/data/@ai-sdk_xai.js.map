{
  "version": 3,
  "sources": ["../../@ai-sdk/openai-compatible/src/openai-compatible-chat-language-model.ts", "../../@ai-sdk/openai-compatible/src/convert-to-openai-compatible-chat-messages.ts", "../../@ai-sdk/openai-compatible/src/get-response-metadata.ts", "../../@ai-sdk/openai-compatible/src/map-openai-compatible-finish-reason.ts", "../../@ai-sdk/openai-compatible/src/openai-compatible-error.ts", "../../@ai-sdk/openai-compatible/src/openai-compatible-prepare-tools.ts", "../../@ai-sdk/openai-compatible/src/openai-compatible-completion-language-model.ts", "../../@ai-sdk/openai-compatible/src/convert-to-openai-compatible-completion-prompt.ts", "../../@ai-sdk/openai-compatible/src/openai-compatible-embedding-model.ts", "../../@ai-sdk/openai-compatible/src/openai-compatible-image-model.ts", "../../@ai-sdk/openai-compatible/src/openai-compatible-provider.ts", "../../@ai-sdk/xai/src/xai-provider.ts", "../../@ai-sdk/xai/src/xai-chat-settings.ts", "../../@ai-sdk/xai/src/xai-error.ts"],
  "sourcesContent": ["import {\n  APICallError,\n  InvalidResponseDataError,\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  LanguageModelV1FinishReason,\n  LanguageModelV1ObjectGenerationMode,\n  LanguageModelV1ProviderMetadata,\n  LanguageModelV1StreamPart,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  generateId,\n  isParsableJson,\n  ParseResult,\n  postJsonToApi,\n  ResponseHandler,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { convertToOpenAICompatibleChatMessages } from './convert-to-openai-compatible-chat-messages';\nimport { getResponseMetadata } from './get-response-metadata';\nimport { mapOpenAICompatibleFinishReason } from './map-openai-compatible-finish-reason';\nimport {\n  OpenAICompatibleChatModelId,\n  OpenAICompatibleChatSettings,\n} from './openai-compatible-chat-settings';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\nimport { prepareTools } from './openai-compatible-prepare-tools';\nimport { MetadataExtractor } from './openai-compatible-metadata-extractor';\n\nexport type OpenAICompatibleChatConfig = {\n  provider: string;\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n  includeUsage?: boolean;\n  errorStructure?: ProviderErrorStructure<any>;\n  metadataExtractor?: MetadataExtractor;\n\n  /**\nDefault object generation mode that should be used with this model when\nno mode is specified. Should be the mode with the best results for this\nmodel. `undefined` can be specified if object generation is not supported.\n  */\n  defaultObjectGenerationMode?: LanguageModelV1ObjectGenerationMode;\n\n  /**\n   * Whether the model supports structured outputs.\n   */\n  supportsStructuredOutputs?: boolean;\n};\n\nexport class OpenAICompatibleChatLanguageModel implements LanguageModelV1 {\n  readonly specificationVersion = 'v1';\n\n  readonly supportsStructuredOutputs: boolean;\n\n  readonly modelId: OpenAICompatibleChatModelId;\n  readonly settings: OpenAICompatibleChatSettings;\n\n  private readonly config: OpenAICompatibleChatConfig;\n  private readonly failedResponseHandler: ResponseHandler<APICallError>;\n  private readonly chunkSchema; // type inferred via constructor\n\n  constructor(\n    modelId: OpenAICompatibleChatModelId,\n    settings: OpenAICompatibleChatSettings,\n    config: OpenAICompatibleChatConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n\n    // initialize error handling:\n    const errorStructure =\n      config.errorStructure ?? defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleChatChunkSchema(\n      errorStructure.errorSchema,\n    );\n    this.failedResponseHandler = createJsonErrorResponseHandler(errorStructure);\n\n    this.supportsStructuredOutputs = config.supportsStructuredOutputs ?? false;\n  }\n\n  get defaultObjectGenerationMode(): 'json' | 'tool' | undefined {\n    return this.config.defaultObjectGenerationMode;\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  private get providerOptionsName(): string {\n    return this.config.provider.split('.')[0].trim();\n  }\n\n  private getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    providerMetadata,\n    stopSequences,\n    responseFormat,\n    seed,\n  }: Parameters<LanguageModelV1['doGenerate']>[0]) {\n    const type = mode.type;\n\n    const warnings: LanguageModelV1CallWarning[] = [];\n\n    if (topK != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'topK',\n      });\n    }\n\n    if (\n      responseFormat?.type === 'json' &&\n      responseFormat.schema != null &&\n      !this.supportsStructuredOutputs\n    ) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'responseFormat',\n        details:\n          'JSON response format schema is only supported with structuredOutputs',\n      });\n    }\n\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n\n      // model specific settings:\n      user: this.settings.user,\n\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      response_format:\n        responseFormat?.type === 'json'\n          ? this.supportsStructuredOutputs === true &&\n            responseFormat.schema != null\n            ? {\n                type: 'json_schema',\n                json_schema: {\n                  schema: responseFormat.schema,\n                  name: responseFormat.name ?? 'response',\n                  description: responseFormat.description,\n                },\n              }\n            : { type: 'json_object' }\n          : undefined,\n\n      stop: stopSequences,\n      seed,\n      ...providerMetadata?.[this.providerOptionsName],\n\n      reasoning_effort:\n        providerMetadata?.[this.providerOptionsName]?.reasoningEffort ??\n        providerMetadata?.['openai-compatible']?.reasoningEffort,\n\n      // messages:\n      messages: convertToOpenAICompatibleChatMessages(prompt),\n    };\n\n    switch (type) {\n      case 'regular': {\n        const { tools, tool_choice, toolWarnings } = prepareTools({\n          mode,\n          structuredOutputs: this.supportsStructuredOutputs,\n        });\n\n        return {\n          args: { ...baseArgs, tools, tool_choice },\n          warnings: [...warnings, ...toolWarnings],\n        };\n      }\n\n      case 'object-json': {\n        return {\n          args: {\n            ...baseArgs,\n            response_format:\n              this.supportsStructuredOutputs === true && mode.schema != null\n                ? {\n                    type: 'json_schema',\n                    json_schema: {\n                      schema: mode.schema,\n                      name: mode.name ?? 'response',\n                      description: mode.description,\n                    },\n                  }\n                : { type: 'json_object' },\n          },\n          warnings,\n        };\n      }\n\n      case 'object-tool': {\n        return {\n          args: {\n            ...baseArgs,\n            tool_choice: {\n              type: 'function',\n              function: { name: mode.tool.name },\n            },\n            tools: [\n              {\n                type: 'function',\n                function: {\n                  name: mode.tool.name,\n                  description: mode.tool.description,\n                  parameters: mode.tool.parameters,\n                },\n              },\n            ],\n          },\n          warnings,\n        };\n      }\n\n      default: {\n        const _exhaustiveCheck: never = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV1['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>> {\n    const { args, warnings } = this.getArgs({ ...options });\n\n    const body = JSON.stringify(args);\n\n    const {\n      responseHeaders,\n      value: responseBody,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/chat/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        OpenAICompatibleChatResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { messages: rawPrompt, ...rawSettings } = args;\n    const choice = responseBody.choices[0];\n\n    // provider metadata:\n    const providerMetadata: LanguageModelV1ProviderMetadata = {\n      [this.providerOptionsName]: {},\n      ...this.config.metadataExtractor?.extractMetadata?.({\n        parsedBody: rawResponse,\n      }),\n    };\n    const completionTokenDetails =\n      responseBody.usage?.completion_tokens_details;\n    const promptTokenDetails = responseBody.usage?.prompt_tokens_details;\n    if (completionTokenDetails?.reasoning_tokens != null) {\n      providerMetadata[this.providerOptionsName].reasoningTokens =\n        completionTokenDetails?.reasoning_tokens;\n    }\n    if (completionTokenDetails?.accepted_prediction_tokens != null) {\n      providerMetadata[this.providerOptionsName].acceptedPredictionTokens =\n        completionTokenDetails?.accepted_prediction_tokens;\n    }\n    if (completionTokenDetails?.rejected_prediction_tokens != null) {\n      providerMetadata[this.providerOptionsName].rejectedPredictionTokens =\n        completionTokenDetails?.rejected_prediction_tokens;\n    }\n    if (promptTokenDetails?.cached_tokens != null) {\n      providerMetadata[this.providerOptionsName].cachedPromptTokens =\n        promptTokenDetails?.cached_tokens;\n    }\n\n    return {\n      text: choice.message.content ?? undefined,\n      reasoning: choice.message.reasoning_content ?? undefined,\n      toolCalls: choice.message.tool_calls?.map(toolCall => ({\n        toolCallType: 'function',\n        toolCallId: toolCall.id ?? generateId(),\n        toolName: toolCall.function.name,\n        args: toolCall.function.arguments!,\n      })),\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      usage: {\n        promptTokens: responseBody.usage?.prompt_tokens ?? NaN,\n        completionTokens: responseBody.usage?.completion_tokens ?? NaN,\n      },\n      providerMetadata,\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      response: getResponseMetadata(responseBody),\n      warnings,\n      request: { body },\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV1['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>> {\n    if (this.settings.simulateStreaming) {\n      const result = await this.doGenerate(options);\n      const simulatedStream = new ReadableStream<LanguageModelV1StreamPart>({\n        start(controller) {\n          controller.enqueue({ type: 'response-metadata', ...result.response });\n          if (result.reasoning) {\n            if (Array.isArray(result.reasoning)) {\n              for (const part of result.reasoning) {\n                if (part.type === 'text') {\n                  controller.enqueue({\n                    type: 'reasoning',\n                    textDelta: part.text,\n                  });\n                }\n              }\n            } else {\n              controller.enqueue({\n                type: 'reasoning',\n                textDelta: result.reasoning,\n              });\n            }\n          }\n          if (result.text) {\n            controller.enqueue({\n              type: 'text-delta',\n              textDelta: result.text,\n            });\n          }\n          if (result.toolCalls) {\n            for (const toolCall of result.toolCalls) {\n              controller.enqueue({\n                type: 'tool-call',\n                ...toolCall,\n              });\n            }\n          }\n          controller.enqueue({\n            type: 'finish',\n            finishReason: result.finishReason,\n            usage: result.usage,\n            logprobs: result.logprobs,\n            providerMetadata: result.providerMetadata,\n          });\n          controller.close();\n        },\n      });\n      return {\n        stream: simulatedStream,\n        rawCall: result.rawCall,\n        rawResponse: result.rawResponse,\n        warnings: result.warnings,\n      };\n    }\n\n    const { args, warnings } = this.getArgs({ ...options });\n\n    const body = {\n      ...args,\n      stream: true,\n\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.includeUsage\n        ? { include_usage: true }\n        : undefined,\n    };\n\n    const metadataExtractor =\n      this.config.metadataExtractor?.createStreamExtractor();\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/chat/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        this.chunkSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { messages: rawPrompt, ...rawSettings } = args;\n\n    const toolCalls: Array<{\n      id: string;\n      type: 'function';\n      function: {\n        name: string;\n        arguments: string;\n      };\n      hasFinished: boolean;\n    }> = [];\n\n    let finishReason: LanguageModelV1FinishReason = 'unknown';\n    let usage: {\n      completionTokens: number | undefined;\n      completionTokensDetails: {\n        reasoningTokens: number | undefined;\n        acceptedPredictionTokens: number | undefined;\n        rejectedPredictionTokens: number | undefined;\n      };\n      promptTokens: number | undefined;\n      promptTokensDetails: {\n        cachedTokens: number | undefined;\n      };\n    } = {\n      completionTokens: undefined,\n      completionTokensDetails: {\n        reasoningTokens: undefined,\n        acceptedPredictionTokens: undefined,\n        rejectedPredictionTokens: undefined,\n      },\n      promptTokens: undefined,\n      promptTokensDetails: {\n        cachedTokens: undefined,\n      },\n    };\n    let isFirstChunk = true;\n    let providerOptionsName = this.providerOptionsName;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof this.chunkSchema>>,\n          LanguageModelV1StreamPart\n        >({\n          // TODO we lost type safety on Chunk, most likely due to the error schema. MUST FIX\n          transform(chunk, controller) {\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n\n            metadataExtractor?.processChunk(chunk.rawValue);\n\n            // handle error chunks:\n            if ('error' in value) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: value.error.message });\n              return;\n            }\n\n            if (isFirstChunk) {\n              isFirstChunk = false;\n\n              controller.enqueue({\n                type: 'response-metadata',\n                ...getResponseMetadata(value),\n              });\n            }\n\n            if (value.usage != null) {\n              const {\n                prompt_tokens,\n                completion_tokens,\n                prompt_tokens_details,\n                completion_tokens_details,\n              } = value.usage;\n\n              usage.promptTokens = prompt_tokens ?? undefined;\n              usage.completionTokens = completion_tokens ?? undefined;\n\n              if (completion_tokens_details?.reasoning_tokens != null) {\n                usage.completionTokensDetails.reasoningTokens =\n                  completion_tokens_details?.reasoning_tokens;\n              }\n              if (\n                completion_tokens_details?.accepted_prediction_tokens != null\n              ) {\n                usage.completionTokensDetails.acceptedPredictionTokens =\n                  completion_tokens_details?.accepted_prediction_tokens;\n              }\n              if (\n                completion_tokens_details?.rejected_prediction_tokens != null\n              ) {\n                usage.completionTokensDetails.rejectedPredictionTokens =\n                  completion_tokens_details?.rejected_prediction_tokens;\n              }\n              if (prompt_tokens_details?.cached_tokens != null) {\n                usage.promptTokensDetails.cachedTokens =\n                  prompt_tokens_details?.cached_tokens;\n              }\n            }\n\n            const choice = value.choices[0];\n\n            if (choice?.finish_reason != null) {\n              finishReason = mapOpenAICompatibleFinishReason(\n                choice.finish_reason,\n              );\n            }\n\n            if (choice?.delta == null) {\n              return;\n            }\n\n            const delta = choice.delta;\n\n            // enqueue reasoning before text deltas:\n            if (delta.reasoning_content != null) {\n              controller.enqueue({\n                type: 'reasoning',\n                textDelta: delta.reasoning_content,\n              });\n            }\n\n            if (delta.content != null) {\n              controller.enqueue({\n                type: 'text-delta',\n                textDelta: delta.content,\n              });\n            }\n\n            if (delta.tool_calls != null) {\n              for (const toolCallDelta of delta.tool_calls) {\n                const index = toolCallDelta.index;\n\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.type !== 'function') {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function' type.`,\n                    });\n                  }\n\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`,\n                    });\n                  }\n\n                  if (toolCallDelta.function?.name == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`,\n                    });\n                  }\n\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: 'function',\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: toolCallDelta.function.arguments ?? '',\n                    },\n                    hasFinished: false,\n                  };\n\n                  const toolCall = toolCalls[index];\n\n                  if (\n                    toolCall.function?.name != null &&\n                    toolCall.function?.arguments != null\n                  ) {\n                    // send delta if the argument text has already started:\n                    if (toolCall.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: 'tool-call-delta',\n                        toolCallType: 'function',\n                        toolCallId: toolCall.id,\n                        toolName: toolCall.function.name,\n                        argsTextDelta: toolCall.function.arguments,\n                      });\n                    }\n\n                    // check if tool call is complete\n                    // (some providers send the full tool call in one chunk):\n                    if (isParsableJson(toolCall.function.arguments)) {\n                      controller.enqueue({\n                        type: 'tool-call',\n                        toolCallType: 'function',\n                        toolCallId: toolCall.id ?? generateId(),\n                        toolName: toolCall.function.name,\n                        args: toolCall.function.arguments,\n                      });\n                      toolCall.hasFinished = true;\n                    }\n                  }\n\n                  continue;\n                }\n\n                // existing tool call, merge if not finished\n                const toolCall = toolCalls[index];\n\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n\n                if (toolCallDelta.function?.arguments != null) {\n                  toolCall.function!.arguments +=\n                    toolCallDelta.function?.arguments ?? '';\n                }\n\n                // send delta\n                controller.enqueue({\n                  type: 'tool-call-delta',\n                  toolCallType: 'function',\n                  toolCallId: toolCall.id,\n                  toolName: toolCall.function.name,\n                  argsTextDelta: toolCallDelta.function.arguments ?? '',\n                });\n\n                // check if tool call is complete\n                if (\n                  toolCall.function?.name != null &&\n                  toolCall.function?.arguments != null &&\n                  isParsableJson(toolCall.function.arguments)\n                ) {\n                  controller.enqueue({\n                    type: 'tool-call',\n                    toolCallType: 'function',\n                    toolCallId: toolCall.id ?? generateId(),\n                    toolName: toolCall.function.name,\n                    args: toolCall.function.arguments,\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n          },\n\n          flush(controller) {\n            const providerMetadata: LanguageModelV1ProviderMetadata = {\n              [providerOptionsName]: {},\n              ...metadataExtractor?.buildMetadata(),\n            };\n            if (usage.completionTokensDetails.reasoningTokens != null) {\n              providerMetadata[providerOptionsName].reasoningTokens =\n                usage.completionTokensDetails.reasoningTokens;\n            }\n            if (\n              usage.completionTokensDetails.acceptedPredictionTokens != null\n            ) {\n              providerMetadata[providerOptionsName].acceptedPredictionTokens =\n                usage.completionTokensDetails.acceptedPredictionTokens;\n            }\n            if (\n              usage.completionTokensDetails.rejectedPredictionTokens != null\n            ) {\n              providerMetadata[providerOptionsName].rejectedPredictionTokens =\n                usage.completionTokensDetails.rejectedPredictionTokens;\n            }\n            if (usage.promptTokensDetails.cachedTokens != null) {\n              providerMetadata[providerOptionsName].cachedPromptTokens =\n                usage.promptTokensDetails.cachedTokens;\n            }\n\n            controller.enqueue({\n              type: 'finish',\n              finishReason,\n              usage: {\n                promptTokens: usage.promptTokens ?? NaN,\n                completionTokens: usage.completionTokens ?? NaN,\n              },\n              providerMetadata,\n            });\n          },\n        }),\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body: JSON.stringify(body) },\n    };\n  }\n}\n\nconst openaiCompatibleTokenUsageSchema = z\n  .object({\n    prompt_tokens: z.number().nullish(),\n    completion_tokens: z.number().nullish(),\n    prompt_tokens_details: z\n      .object({\n        cached_tokens: z.number().nullish(),\n      })\n      .nullish(),\n    completion_tokens_details: z\n      .object({\n        reasoning_tokens: z.number().nullish(),\n        accepted_prediction_tokens: z.number().nullish(),\n        rejected_prediction_tokens: z.number().nullish(),\n      })\n      .nullish(),\n  })\n  .nullish();\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst OpenAICompatibleChatResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      message: z.object({\n        role: z.literal('assistant').nullish(),\n        content: z.string().nullish(),\n        reasoning_content: z.string().nullish(),\n        tool_calls: z\n          .array(\n            z.object({\n              id: z.string().nullish(),\n              type: z.literal('function'),\n              function: z.object({\n                name: z.string(),\n                arguments: z.string(),\n              }),\n            }),\n          )\n          .nullish(),\n      }),\n      finish_reason: z.string().nullish(),\n    }),\n  ),\n  usage: openaiCompatibleTokenUsageSchema,\n});\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst createOpenAICompatibleChatChunkSchema = <ERROR_SCHEMA extends z.ZodType>(\n  errorSchema: ERROR_SCHEMA,\n) =>\n  z.union([\n    z.object({\n      id: z.string().nullish(),\n      created: z.number().nullish(),\n      model: z.string().nullish(),\n      choices: z.array(\n        z.object({\n          delta: z\n            .object({\n              role: z.enum(['assistant']).nullish(),\n              content: z.string().nullish(),\n              reasoning_content: z.string().nullish(),\n              tool_calls: z\n                .array(\n                  z.object({\n                    index: z.number().optional(),\n                    id: z.string().nullish(),\n                    type: z.literal('function').nullish(),\n                    function: z.object({\n                      name: z.string().nullish(),\n                      arguments: z.string().nullish(),\n                    }),\n                  }),\n                )\n                .nullish(),\n            })\n            .nullish(),\n          finish_reason: z.string().nullish(),\n        }),\n      ),\n      usage: openaiCompatibleTokenUsageSchema,\n    }),\n    errorSchema,\n  ]);\n", "import {\n  LanguageModelV1Prompt,\n  LanguageModelV1ProviderMetadata,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport { convertUint8ArrayToBase64 } from '@ai-sdk/provider-utils';\nimport { OpenAICompatibleChatPrompt } from './openai-compatible-api-types';\n\nfunction getOpenAIMetadata(message: {\n  providerMetadata?: LanguageModelV1ProviderMetadata;\n}) {\n  return message?.providerMetadata?.openaiCompatible ?? {};\n}\n\nexport function convertToOpenAICompatibleChatMessages(\n  prompt: LanguageModelV1Prompt,\n): OpenAICompatibleChatPrompt {\n  const messages: OpenAICompatibleChatPrompt = [];\n  for (const { role, content, ...message } of prompt) {\n    const metadata = getOpenAIMetadata({ ...message });\n    switch (role) {\n      case 'system': {\n        messages.push({ role: 'system', content, ...metadata });\n        break;\n      }\n\n      case 'user': {\n        if (content.length === 1 && content[0].type === 'text') {\n          messages.push({\n            role: 'user',\n            content: content[0].text,\n            ...getOpenAIMetadata(content[0]),\n          });\n          break;\n        }\n\n        messages.push({\n          role: 'user',\n          content: content.map(part => {\n            const partMetadata = getOpenAIMetadata(part);\n            switch (part.type) {\n              case 'text': {\n                return { type: 'text', text: part.text, ...partMetadata };\n              }\n              case 'image': {\n                return {\n                  type: 'image_url',\n                  image_url: {\n                    url:\n                      part.image instanceof URL\n                        ? part.image.toString()\n                        : `data:${\n                            part.mimeType ?? 'image/jpeg'\n                          };base64,${convertUint8ArrayToBase64(part.image)}`,\n                  },\n                  ...partMetadata,\n                };\n              }\n              case 'file': {\n                throw new UnsupportedFunctionalityError({\n                  functionality: 'File content parts in user messages',\n                });\n              }\n            }\n          }),\n          ...metadata,\n        });\n\n        break;\n      }\n\n      case 'assistant': {\n        let text = '';\n        const toolCalls: Array<{\n          id: string;\n          type: 'function';\n          function: { name: string; arguments: string };\n        }> = [];\n\n        for (const part of content) {\n          const partMetadata = getOpenAIMetadata(part);\n          switch (part.type) {\n            case 'text': {\n              text += part.text;\n              break;\n            }\n            case 'tool-call': {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: 'function',\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.args),\n                },\n                ...partMetadata,\n              });\n              break;\n            }\n          }\n        }\n\n        messages.push({\n          role: 'assistant',\n          content: text,\n          tool_calls: toolCalls.length > 0 ? toolCalls : undefined,\n          ...metadata,\n        });\n\n        break;\n      }\n\n      case 'tool': {\n        for (const toolResponse of content) {\n          const toolResponseMetadata = getOpenAIMetadata(toolResponse);\n          messages.push({\n            role: 'tool',\n            tool_call_id: toolResponse.toolCallId,\n            content: JSON.stringify(toolResponse.result),\n            ...toolResponseMetadata,\n          });\n        }\n        break;\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  return messages;\n}\n", "export function getResponseMetadata({\n  id,\n  model,\n  created,\n}: {\n  id?: string | undefined | null;\n  created?: number | undefined | null;\n  model?: string | undefined | null;\n}) {\n  return {\n    id: id ?? undefined,\n    modelId: model ?? undefined,\n    timestamp: created != null ? new Date(created * 1000) : undefined,\n  };\n}\n", "import { LanguageModelV1FinishReason } from '@ai-sdk/provider';\n\nexport function mapOpenAICompatibleFinishReason(\n  finishReason: string | null | undefined,\n): LanguageModelV1FinishReason {\n  switch (finishReason) {\n    case 'stop':\n      return 'stop';\n    case 'length':\n      return 'length';\n    case 'content_filter':\n      return 'content-filter';\n    case 'function_call':\n    case 'tool_calls':\n      return 'tool-calls';\n    default:\n      return 'unknown';\n  }\n}\n", "import { z, ZodSchema } from 'zod';\n\nexport const openaiCompatibleErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish(),\n  }),\n});\n\nexport type OpenAICompatibleErrorData = z.infer<\n  typeof openaiCompatibleErrorDataSchema\n>;\n\nexport type ProviderErrorStructure<T> = {\n  errorSchema: ZodSchema<T>;\n  errorToMessage: (error: T) => string;\n  isRetryable?: (response: Response, error?: T) => boolean;\n};\n\nexport const defaultOpenAICompatibleErrorStructure: ProviderErrorStructure<OpenAICompatibleErrorData> =\n  {\n    errorSchema: openaiCompatibleErrorDataSchema,\n    errorToMessage: data => data.error.message,\n  };\n", "import {\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\n\nexport function prepareTools({\n  mode,\n  structuredOutputs,\n}: {\n  mode: Parameters<LanguageModelV1['doGenerate']>[0]['mode'] & {\n    type: 'regular';\n  };\n  structuredOutputs: boolean;\n}): {\n  tools:\n    | undefined\n    | Array<{\n        type: 'function';\n        function: {\n          name: string;\n          description: string | undefined;\n          parameters: unknown;\n        };\n      }>;\n  tool_choice:\n    | { type: 'function'; function: { name: string } }\n    | 'auto'\n    | 'none'\n    | 'required'\n    | undefined;\n  toolWarnings: LanguageModelV1CallWarning[];\n} {\n  // when the tools array is empty, change it to undefined to prevent errors:\n  const tools = mode.tools?.length ? mode.tools : undefined;\n  const toolWarnings: LanguageModelV1CallWarning[] = [];\n\n  if (tools == null) {\n    return { tools: undefined, tool_choice: undefined, toolWarnings };\n  }\n\n  const toolChoice = mode.toolChoice;\n\n  const openaiCompatTools: Array<{\n    type: 'function';\n    function: {\n      name: string;\n      description: string | undefined;\n      parameters: unknown;\n    };\n  }> = [];\n\n  for (const tool of tools) {\n    if (tool.type === 'provider-defined') {\n      toolWarnings.push({ type: 'unsupported-tool', tool });\n    } else {\n      openaiCompatTools.push({\n        type: 'function',\n        function: {\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters,\n        },\n      });\n    }\n  }\n\n  if (toolChoice == null) {\n    return { tools: openaiCompatTools, tool_choice: undefined, toolWarnings };\n  }\n\n  const type = toolChoice.type;\n\n  switch (type) {\n    case 'auto':\n    case 'none':\n    case 'required':\n      return { tools: openaiCompatTools, tool_choice: type, toolWarnings };\n    case 'tool':\n      return {\n        tools: openaiCompatTools,\n        tool_choice: {\n          type: 'function',\n          function: {\n            name: toolChoice.toolName,\n          },\n        },\n        toolWarnings,\n      };\n    default: {\n      const _exhaustiveCheck: never = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`,\n      });\n    }\n  }\n}\n", "import {\n  APICallError,\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  LanguageModelV1FinishReason,\n  LanguageModelV1StreamPart,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  ParseResult,\n  postJsonToApi,\n  ResponseHandler,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { convertToOpenAICompatibleCompletionPrompt } from './convert-to-openai-compatible-completion-prompt';\nimport { getResponseMetadata } from './get-response-metadata';\nimport { mapOpenAICompatibleFinishReason } from './map-openai-compatible-finish-reason';\nimport {\n  OpenAICompatibleCompletionModelId,\n  OpenAICompatibleCompletionSettings,\n} from './openai-compatible-completion-settings';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\n\ntype OpenAICompatibleCompletionConfig = {\n  provider: string;\n  includeUsage?: boolean;\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n};\n\nexport class OpenAICompatibleCompletionLanguageModel\n  implements LanguageModelV1\n{\n  readonly specificationVersion = 'v1';\n  readonly defaultObjectGenerationMode = undefined;\n\n  readonly modelId: OpenAICompatibleCompletionModelId;\n  readonly settings: OpenAICompatibleCompletionSettings;\n\n  private readonly config: OpenAICompatibleCompletionConfig;\n  private readonly failedResponseHandler: ResponseHandler<APICallError>;\n  private readonly chunkSchema; // type inferred via constructor\n\n  constructor(\n    modelId: OpenAICompatibleCompletionModelId,\n    settings: OpenAICompatibleCompletionSettings,\n    config: OpenAICompatibleCompletionConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n\n    // initialize error handling:\n    const errorStructure =\n      config.errorStructure ?? defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleCompletionChunkSchema(\n      errorStructure.errorSchema,\n    );\n    this.failedResponseHandler = createJsonErrorResponseHandler(errorStructure);\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  private get providerOptionsName(): string {\n    return this.config.provider.split('.')[0].trim();\n  }\n\n  private getArgs({\n    mode,\n    inputFormat,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    seed,\n    providerMetadata,\n  }: Parameters<LanguageModelV1['doGenerate']>[0]) {\n    const type = mode.type;\n\n    const warnings: LanguageModelV1CallWarning[] = [];\n\n    if (topK != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'topK',\n      });\n    }\n\n    if (responseFormat != null && responseFormat.type !== 'text') {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'responseFormat',\n        details: 'JSON response format is not supported.',\n      });\n    }\n\n    const { prompt: completionPrompt, stopSequences } =\n      convertToOpenAICompatibleCompletionPrompt({ prompt, inputFormat });\n\n    const stop = [...(stopSequences ?? []), ...(userStopSequences ?? [])];\n\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n\n      // model specific settings:\n      echo: this.settings.echo,\n      logit_bias: this.settings.logitBias,\n      suffix: this.settings.suffix,\n      user: this.settings.user,\n\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      seed,\n      ...providerMetadata?.[this.providerOptionsName],\n\n      // prompt:\n      prompt: completionPrompt,\n\n      // stop sequences:\n      stop: stop.length > 0 ? stop : undefined,\n    };\n\n    switch (type) {\n      case 'regular': {\n        if (mode.tools?.length) {\n          throw new UnsupportedFunctionalityError({\n            functionality: 'tools',\n          });\n        }\n\n        if (mode.toolChoice) {\n          throw new UnsupportedFunctionalityError({\n            functionality: 'toolChoice',\n          });\n        }\n\n        return { args: baseArgs, warnings };\n      }\n\n      case 'object-json': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'object-json mode',\n        });\n      }\n\n      case 'object-tool': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'object-tool mode',\n        });\n      }\n\n      default: {\n        const _exhaustiveCheck: never = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV1['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>> {\n    const { args, warnings } = this.getArgs(options);\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiCompatibleCompletionResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { prompt: rawPrompt, ...rawSettings } = args;\n    const choice = response.choices[0];\n\n    return {\n      text: choice.text,\n      usage: {\n        promptTokens: response.usage?.prompt_tokens ?? NaN,\n        completionTokens: response.usage?.completion_tokens ?? NaN,\n      },\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      response: getResponseMetadata(response),\n      warnings,\n      request: { body: JSON.stringify(args) },\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV1['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>> {\n    const { args, warnings } = this.getArgs(options);\n\n    const body = {\n      ...args,\n      stream: true,\n\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.includeUsage\n        ? { include_usage: true }\n        : undefined,\n    };\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        this.chunkSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { prompt: rawPrompt, ...rawSettings } = args;\n\n    let finishReason: LanguageModelV1FinishReason = 'unknown';\n    let usage: { promptTokens: number; completionTokens: number } = {\n      promptTokens: Number.NaN,\n      completionTokens: Number.NaN,\n    };\n    let isFirstChunk = true;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof this.chunkSchema>>,\n          LanguageModelV1StreamPart\n        >({\n          transform(chunk, controller) {\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n\n            const value = chunk.value;\n\n            // handle error chunks:\n            if ('error' in value) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: value.error });\n              return;\n            }\n\n            if (isFirstChunk) {\n              isFirstChunk = false;\n\n              controller.enqueue({\n                type: 'response-metadata',\n                ...getResponseMetadata(value),\n              });\n            }\n\n            if (value.usage != null) {\n              usage = {\n                promptTokens: value.usage.prompt_tokens,\n                completionTokens: value.usage.completion_tokens,\n              };\n            }\n\n            const choice = value.choices[0];\n\n            if (choice?.finish_reason != null) {\n              finishReason = mapOpenAICompatibleFinishReason(\n                choice.finish_reason,\n              );\n            }\n\n            if (choice?.text != null) {\n              controller.enqueue({\n                type: 'text-delta',\n                textDelta: choice.text,\n              });\n            }\n          },\n\n          flush(controller) {\n            controller.enqueue({\n              type: 'finish',\n              finishReason,\n              usage,\n            });\n          },\n        }),\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body: JSON.stringify(body) },\n    };\n  }\n}\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiCompatibleCompletionResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      text: z.string(),\n      finish_reason: z.string(),\n    }),\n  ),\n  usage: z\n    .object({\n      prompt_tokens: z.number(),\n      completion_tokens: z.number(),\n    })\n    .nullish(),\n});\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst createOpenAICompatibleCompletionChunkSchema = <\n  ERROR_SCHEMA extends z.ZodType,\n>(\n  errorSchema: ERROR_SCHEMA,\n) =>\n  z.union([\n    z.object({\n      id: z.string().nullish(),\n      created: z.number().nullish(),\n      model: z.string().nullish(),\n      choices: z.array(\n        z.object({\n          text: z.string(),\n          finish_reason: z.string().nullish(),\n          index: z.number(),\n        }),\n      ),\n      usage: z\n        .object({\n          prompt_tokens: z.number(),\n          completion_tokens: z.number(),\n        })\n        .nullish(),\n    }),\n    errorSchema,\n  ]);\n", "import {\n  InvalidPromptError,\n  LanguageModelV1Prompt,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\n\nexport function convertToOpenAICompatibleCompletionPrompt({\n  prompt,\n  inputFormat,\n  user = 'user',\n  assistant = 'assistant',\n}: {\n  prompt: LanguageModelV1Prompt;\n  inputFormat: 'prompt' | 'messages';\n  user?: string;\n  assistant?: string;\n}): {\n  prompt: string;\n  stopSequences?: string[];\n} {\n  // When the user supplied a prompt input, we don't transform it:\n  if (\n    inputFormat === 'prompt' &&\n    prompt.length === 1 &&\n    prompt[0].role === 'user' &&\n    prompt[0].content.length === 1 &&\n    prompt[0].content[0].type === 'text'\n  ) {\n    return { prompt: prompt[0].content[0].text };\n  }\n\n  // otherwise transform to a chat message format:\n  let text = '';\n\n  // if first message is a system message, add it to the text:\n  if (prompt[0].role === 'system') {\n    text += `${prompt[0].content}\\n\\n`;\n    prompt = prompt.slice(1);\n  }\n\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case 'system': {\n        throw new InvalidPromptError({\n          message: 'Unexpected system message in prompt: ${content}',\n          prompt,\n        });\n      }\n\n      case 'user': {\n        const userMessage = content\n          .map(part => {\n            switch (part.type) {\n              case 'text': {\n                return part.text;\n              }\n              case 'image': {\n                throw new UnsupportedFunctionalityError({\n                  functionality: 'images',\n                });\n              }\n            }\n          })\n          .join('');\n\n        text += `${user}:\\n${userMessage}\\n\\n`;\n        break;\n      }\n\n      case 'assistant': {\n        const assistantMessage = content\n          .map(part => {\n            switch (part.type) {\n              case 'text': {\n                return part.text;\n              }\n              case 'tool-call': {\n                throw new UnsupportedFunctionalityError({\n                  functionality: 'tool-call messages',\n                });\n              }\n            }\n          })\n          .join('');\n\n        text += `${assistant}:\\n${assistantMessage}\\n\\n`;\n        break;\n      }\n\n      case 'tool': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'tool messages',\n        });\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  // Assistant message prefix:\n  text += `${assistant}:\\n`;\n\n  return {\n    prompt: text,\n    stopSequences: [`\\n${user}:`],\n  };\n}\n", "import {\n  EmbeddingModelV1,\n  TooManyEmbeddingValuesForCallError,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport {\n  OpenAICompatibleEmbeddingModelId,\n  OpenAICompatibleEmbeddingSettings,\n} from './openai-compatible-embedding-settings';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\n\ntype OpenAICompatibleEmbeddingConfig = {\n  /**\nOverride the maximum number of embeddings per call.\n   */\n  maxEmbeddingsPerCall?: number;\n\n  /**\nOverride the parallelism of embedding calls.\n  */\n  supportsParallelCalls?: boolean;\n\n  provider: string;\n  url: (options: { modelId: string; path: string }) => string;\n  headers: () => Record<string, string | undefined>;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n};\n\nexport class OpenAICompatibleEmbeddingModel\n  implements EmbeddingModelV1<string>\n{\n  readonly specificationVersion = 'v1';\n  readonly modelId: OpenAICompatibleEmbeddingModelId;\n\n  private readonly config: OpenAICompatibleEmbeddingConfig;\n  private readonly settings: OpenAICompatibleEmbeddingSettings;\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  get maxEmbeddingsPerCall(): number {\n    return this.config.maxEmbeddingsPerCall ?? 2048;\n  }\n\n  get supportsParallelCalls(): boolean {\n    return this.config.supportsParallelCalls ?? true;\n  }\n\n  constructor(\n    modelId: OpenAICompatibleEmbeddingModelId,\n    settings: OpenAICompatibleEmbeddingSettings,\n    config: OpenAICompatibleEmbeddingConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n\n  async doEmbed({\n    values,\n    headers,\n    abortSignal,\n  }: Parameters<EmbeddingModelV1<string>['doEmbed']>[0]): Promise<\n    Awaited<ReturnType<EmbeddingModelV1<string>['doEmbed']>>\n  > {\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values,\n      });\n    }\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/embeddings',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        input: values,\n        encoding_format: 'float',\n        dimensions: this.settings.dimensions,\n        user: this.settings.user,\n      },\n      failedResponseHandler: createJsonErrorResponseHandler(\n        this.config.errorStructure ?? defaultOpenAICompatibleErrorStructure,\n      ),\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiTextEmbeddingResponseSchema,\n      ),\n      abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    return {\n      embeddings: response.data.map(item => item.embedding),\n      usage: response.usage\n        ? { tokens: response.usage.prompt_tokens }\n        : undefined,\n      rawResponse: { headers: responseHeaders },\n    };\n  }\n}\n\n// minimal version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiTextEmbeddingResponseSchema = z.object({\n  data: z.array(z.object({ embedding: z.array(z.number()) })),\n  usage: z.object({ prompt_tokens: z.number() }).nullish(),\n});\n", "import { ImageModelV1, ImageModelV1CallWarning } from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { OpenAICompatibleImageModelId } from './openai-compatible-image-settings';\nimport { OpenAICompatibleImageSettings } from './openai-compatible-image-settings';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\n\nexport type OpenAICompatibleImageModelConfig = {\n  provider: string;\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n  _internal?: {\n    currentDate?: () => Date;\n  };\n};\n\nexport class OpenAICompatibleImageModel implements ImageModelV1 {\n  readonly specificationVersion = 'v1';\n\n  get maxImagesPerCall(): number {\n    return this.settings.maxImagesPerCall ?? 10;\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  constructor(\n    readonly modelId: OpenAICompatibleImageModelId,\n    private readonly settings: OpenAICompatibleImageSettings,\n    private readonly config: OpenAICompatibleImageModelConfig,\n  ) {}\n\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal,\n  }: Parameters<ImageModelV1['doGenerate']>[0]): Promise<\n    Awaited<ReturnType<ImageModelV1['doGenerate']>>\n  > {\n    const warnings: Array<ImageModelV1CallWarning> = [];\n\n    if (aspectRatio != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'aspectRatio',\n        details:\n          'This model does not support aspect ratio. Use `size` instead.',\n      });\n    }\n\n    if (seed != null) {\n      warnings.push({ type: 'unsupported-setting', setting: 'seed' });\n    }\n\n    const currentDate = this.config._internal?.currentDate?.() ?? new Date();\n    const { value: response, responseHeaders } = await postJsonToApi({\n      url: this.config.url({\n        path: '/images/generations',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...(providerOptions.openai ?? {}),\n        response_format: 'b64_json',\n        ...(this.settings.user ? { user: this.settings.user } : {}),\n      },\n      failedResponseHandler: createJsonErrorResponseHandler(\n        this.config.errorStructure ?? defaultOpenAICompatibleErrorStructure,\n      ),\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiCompatibleImageResponseSchema,\n      ),\n      abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    return {\n      images: response.data.map(item => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n      },\n    };\n  }\n}\n\n// minimal version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiCompatibleImageResponseSchema = z.object({\n  data: z.array(z.object({ b64_json: z.string() })),\n});\n", "import {\n  EmbeddingModelV1,\n  ImageModelV1,\n  LanguageModelV1,\n  ProviderV1,\n} from '@ai-sdk/provider';\nimport { FetchFunction, withoutTrailingSlash } from '@ai-sdk/provider-utils';\nimport {\n  OpenAICompatibleChatConfig,\n  OpenAICompatibleChatLanguageModel,\n} from './openai-compatible-chat-language-model';\nimport { OpenAICompatibleChatSettings } from './openai-compatible-chat-settings';\nimport { OpenAICompatibleCompletionLanguageModel } from './openai-compatible-completion-language-model';\nimport { OpenAICompatibleCompletionSettings } from './openai-compatible-completion-settings';\nimport { OpenAICompatibleEmbeddingModel } from './openai-compatible-embedding-model';\nimport { OpenAICompatibleEmbeddingSettings } from './openai-compatible-embedding-settings';\nimport { OpenAICompatibleImageSettings } from './openai-compatible-image-settings';\nimport { OpenAICompatibleImageModel } from './openai-compatible-image-model';\n\nexport interface OpenAICompatibleProvider<\n  CHAT_MODEL_IDS extends string = string,\n  COMPLETION_MODEL_IDS extends string = string,\n  EMBEDDING_MODEL_IDS extends string = string,\n  IMAGE_MODEL_IDS extends string = string,\n> extends Omit<ProviderV1, 'imageModel'> {\n  (\n    modelId: CHAT_MODEL_IDS,\n    settings?: OpenAICompatibleChatSettings,\n    config?: Partial<OpenAICompatibleChatConfig>,\n  ): LanguageModelV1;\n\n  languageModel(\n    modelId: CHAT_MODEL_IDS,\n    settings?: OpenAICompatibleChatSettings,\n    config?: Partial<OpenAICompatibleChatConfig>,\n  ): LanguageModelV1;\n\n  chatModel(\n    modelId: CHAT_MODEL_IDS,\n    settings?: OpenAICompatibleChatSettings,\n    config?: Partial<OpenAICompatibleChatConfig>,\n  ): LanguageModelV1;\n\n  completionModel(\n    modelId: COMPLETION_MODEL_IDS,\n    settings?: OpenAICompatibleCompletionSettings,\n  ): LanguageModelV1;\n\n  textEmbeddingModel(\n    modelId: EMBEDDING_MODEL_IDS,\n    settings?: OpenAICompatibleEmbeddingSettings,\n  ): EmbeddingModelV1<string>;\n\n  imageModel(\n    modelId: IMAGE_MODEL_IDS,\n    settings?: OpenAICompatibleImageSettings,\n  ): ImageModelV1;\n}\n\nexport interface OpenAICompatibleProviderSettings {\n  /**\nBase URL for the API calls.\n   */\n  baseURL: string;\n\n  /**\nProvider name.\n   */\n  name: string;\n\n  /**\nAPI key for authenticating requests. If specified, adds an `Authorization`\nheader to request headers with the value `Bearer <apiKey>`. This will be added\nbefore any headers potentially specified in the `headers` option.\n   */\n  apiKey?: string;\n\n  /**\nOptional custom headers to include in requests. These will be added to request headers\nafter any headers potentially added by use of the `apiKey` option.\n   */\n  headers?: Record<string, string>;\n\n  /**\nOptional custom url query parameters to include in request urls.\n   */\n  queryParams?: Record<string, string>;\n\n  /**\nCustom fetch implementation. You can use it as a middleware to intercept requests,\nor to provide a custom fetch implementation for e.g. testing.\n   */\n  fetch?: FetchFunction;\n}\n\n/**\nCreate an OpenAICompatible provider instance.\n */\nexport function createOpenAICompatible<\n  CHAT_MODEL_IDS extends string,\n  COMPLETION_MODEL_IDS extends string,\n  EMBEDDING_MODEL_IDS extends string,\n  IMAGE_MODEL_IDS extends string,\n>(\n  options: OpenAICompatibleProviderSettings,\n): OpenAICompatibleProvider<\n  CHAT_MODEL_IDS,\n  COMPLETION_MODEL_IDS,\n  EMBEDDING_MODEL_IDS,\n  IMAGE_MODEL_IDS\n> {\n  const baseURL = withoutTrailingSlash(options.baseURL);\n  const providerName = options.name;\n\n  interface CommonModelConfig {\n    provider: string;\n    url: ({ path }: { path: string }) => string;\n    headers: () => Record<string, string>;\n    fetch?: FetchFunction;\n  }\n\n  const getHeaders = () => ({\n    ...(options.apiKey && { Authorization: `Bearer ${options.apiKey}` }),\n    ...options.headers,\n  });\n\n  const getCommonModelConfig = (modelType: string): CommonModelConfig => ({\n    provider: `${providerName}.${modelType}`,\n    url: ({ path }) => {\n      const url = new URL(`${baseURL}${path}`);\n      if (options.queryParams) {\n        url.search = new URLSearchParams(options.queryParams).toString();\n      }\n      return url.toString();\n    },\n    headers: getHeaders,\n    fetch: options.fetch,\n  });\n\n  const createLanguageModel = (\n    modelId: CHAT_MODEL_IDS,\n    settings: OpenAICompatibleChatSettings = {},\n    config?: Partial<OpenAICompatibleChatConfig>,\n  ) => createChatModel(modelId, settings, config);\n\n  const createChatModel = (\n    modelId: CHAT_MODEL_IDS,\n    settings: OpenAICompatibleChatSettings = {},\n    config?: Partial<OpenAICompatibleChatConfig>,\n  ) =>\n    new OpenAICompatibleChatLanguageModel(modelId, settings, {\n      ...getCommonModelConfig('chat'),\n      defaultObjectGenerationMode: 'tool',\n      ...config,\n    });\n\n  const createCompletionModel = (\n    modelId: COMPLETION_MODEL_IDS,\n    settings: OpenAICompatibleCompletionSettings = {},\n  ) =>\n    new OpenAICompatibleCompletionLanguageModel(\n      modelId,\n      settings,\n      getCommonModelConfig('completion'),\n    );\n\n  const createEmbeddingModel = (\n    modelId: EMBEDDING_MODEL_IDS,\n    settings: OpenAICompatibleEmbeddingSettings = {},\n  ) =>\n    new OpenAICompatibleEmbeddingModel(\n      modelId,\n      settings,\n      getCommonModelConfig('embedding'),\n    );\n\n  const createImageModel = (\n    modelId: IMAGE_MODEL_IDS,\n    settings: OpenAICompatibleImageSettings = {},\n  ) =>\n    new OpenAICompatibleImageModel(\n      modelId,\n      settings,\n      getCommonModelConfig('image'),\n    );\n\n  const provider = (\n    modelId: CHAT_MODEL_IDS,\n    settings?: OpenAICompatibleChatSettings,\n    config?: Partial<OpenAICompatibleChatConfig>,\n  ) => createLanguageModel(modelId, settings, config);\n\n  provider.languageModel = createLanguageModel;\n  provider.chatModel = createChatModel;\n  provider.completionModel = createCompletionModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  provider.imageModel = createImageModel;\n\n  return provider as OpenAICompatibleProvider<\n    CHAT_MODEL_IDS,\n    COMPLETION_MODEL_IDS,\n    EMBEDDING_MODEL_IDS,\n    IMAGE_MODEL_IDS\n  >;\n}\n", "import {\n  ImageModelV1,\n  LanguageModelV1,\n  NoSuchModelError,\n  ProviderV1,\n} from '@ai-sdk/provider';\nimport {\n  OpenAICompatibleChatLanguageModel,\n  OpenAICompatibleImageModel,\n  ProviderErrorStructure,\n} from '@ai-sdk/openai-compatible';\nimport {\n  FetchFunction,\n  loadApiKey,\n  withoutTrailingSlash,\n} from '@ai-sdk/provider-utils';\nimport {\n  XaiChatModelId,\n  XaiChatSettings,\n  supportsStructuredOutputs,\n} from './xai-chat-settings';\nimport { XaiImageSettings } from './xai-image-settings';\nimport { XaiImageModelId } from './xai-image-settings';\nimport { XaiErrorData, xaiErrorSchema } from './xai-error';\n\nconst xaiErrorStructure: ProviderErrorStructure<XaiErrorData> = {\n  errorSchema: xaiErrorSchema,\n  errorToMessage: data => data.error,\n};\n\nexport interface XaiProvider extends ProviderV1 {\n  /**\nCreates an Xai chat model for text generation.\n   */\n  (modelId: XaiChatModelId, settings?: XaiChatSettings): LanguageModelV1;\n\n  /**\nCreates an Xai language model for text generation.\n   */\n  languageModel(\n    modelId: XaiChatModelId,\n    settings?: XaiChatSettings,\n  ): LanguageModelV1;\n\n  /**\nCreates an Xai chat model for text generation.\n   */\n  chat: (\n    modelId: XaiChatModelId,\n    settings?: XaiChatSettings,\n  ) => LanguageModelV1;\n\n  /**\nCreates an Xai image model for image generation.\n   */\n  image(modelId: XaiImageModelId, settings?: XaiImageSettings): ImageModelV1;\n\n  /**\nCreates an Xai image model for image generation.\n   */\n  imageModel(\n    modelId: XaiImageModelId,\n    settings?: XaiImageSettings,\n  ): ImageModelV1;\n}\n\nexport interface XaiProviderSettings {\n  /**\nBase URL for the xAI API calls.\n     */\n  baseURL?: string;\n\n  /**\nAPI key for authenticating requests.\n   */\n  apiKey?: string;\n\n  /**\nCustom headers to include in the requests.\n   */\n  headers?: Record<string, string>;\n\n  /**\nCustom fetch implementation. You can use it as a middleware to intercept requests,\nor to provide a custom fetch implementation for e.g. testing.\n  */\n  fetch?: FetchFunction;\n}\n\nexport function createXai(options: XaiProviderSettings = {}): XaiProvider {\n  const baseURL = withoutTrailingSlash(\n    options.baseURL ?? 'https://api.x.ai/v1',\n  );\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: 'XAI_API_KEY',\n      description: 'xAI API key',\n    })}`,\n    ...options.headers,\n  });\n\n  const createLanguageModel = (\n    modelId: XaiChatModelId,\n    settings: XaiChatSettings = {},\n  ) => {\n    const structuredOutputs = supportsStructuredOutputs(modelId);\n    return new OpenAICompatibleChatLanguageModel(modelId, settings, {\n      provider: 'xai.chat',\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n      defaultObjectGenerationMode: structuredOutputs ? 'json' : 'tool',\n      errorStructure: xaiErrorStructure,\n      supportsStructuredOutputs: structuredOutputs,\n      includeUsage: true,\n    });\n  };\n\n  const createImageModel = (\n    modelId: XaiImageModelId,\n    settings: XaiImageSettings = {},\n  ) => {\n    return new OpenAICompatibleImageModel(modelId, settings, {\n      provider: 'xai.image',\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n      errorStructure: xaiErrorStructure,\n    });\n  };\n\n  const provider = (modelId: XaiChatModelId, settings?: XaiChatSettings) =>\n    createLanguageModel(modelId, settings);\n\n  provider.languageModel = createLanguageModel;\n  provider.chat = createLanguageModel;\n  provider.textEmbeddingModel = (modelId: string) => {\n    throw new NoSuchModelError({ modelId, modelType: 'textEmbeddingModel' });\n  };\n  provider.imageModel = createImageModel;\n  provider.image = createImageModel;\n\n  return provider;\n}\n\nexport const xai = createXai();\n", "import { OpenAICompatibleChatSettings } from '@ai-sdk/openai-compatible';\n\n// https://console.x.ai and see \"View models\"\nexport type XaiChatModelId =\n  | 'grok-3'\n  | 'grok-3-latest'\n  | 'grok-3-fast'\n  | 'grok-3-fast-latest'\n  | 'grok-3-mini'\n  | 'grok-3-mini-latest'\n  | 'grok-3-mini-fast'\n  | 'grok-3-mini-fast-latest'\n  | 'grok-2-vision-1212'\n  | 'grok-2-vision'\n  | 'grok-2-vision-latest'\n  | 'grok-2-image-1212'\n  | 'grok-2-image'\n  | 'grok-2-image-latest'\n  | 'grok-2-1212'\n  | 'grok-2'\n  | 'grok-2-latest'\n  | 'grok-vision-beta'\n  | 'grok-beta'\n  | (string & {});\n\nexport interface XaiChatSettings extends OpenAICompatibleChatSettings {}\n\n/**\n * https://docs.x.ai/docs/guides/structured-outputs\n */\nexport function supportsStructuredOutputs(modelId: XaiChatModelId) {\n  return [\n    'grok-3',\n    'grok-3-beta',\n    'grok-3-latest',\n    'grok-3-fast',\n    'grok-3-fast-beta',\n    'grok-3-fast-latest',\n    'grok-3-mini',\n    'grok-3-mini-beta',\n    'grok-3-mini-latest',\n    'grok-3-mini-fast',\n    'grok-3-mini-fast-beta',\n    'grok-3-mini-fast-latest',\n    'grok-2-1212',\n    'grok-2-vision-1212',\n  ].includes(modelId);\n}\n", "import { z } from 'zod';\n\n// Add error schema and structure\nexport const xaiErrorSchema = z.object({\n  code: z.string(),\n  error: z.string(),\n});\n\nexport type XaiErrorData = z.infer<typeof xaiErrorSchema>;\n"],
  "mappings": ";;;;;;;;;;;;;;;;;;;;;ACQA,SAAS,kBAAkB,SAExB;AAVH,MAAA,IAAA;AAWE,UAAO,MAAA,KAAA,WAAA,OAAA,SAAA,QAAS,qBAAT,OAAA,SAAA,GAA2B,qBAA3B,OAAA,KAA+C,CAAC;AACzD;AAEO,SAAS,sCACd,QAC4B;AAC5B,QAAM,WAAuC,CAAC;AAC9C,aAAW,EAAE,MAAM,SAAS,GAAG,QAAQ,KAAK,QAAQ;AAClD,UAAM,WAAW,kBAAkB,EAAE,GAAG,QAAQ,CAAC;AACjD,YAAQ,MAAM;MACZ,KAAK,UAAU;AACb,iBAAS,KAAK,EAAE,MAAM,UAAU,SAAS,GAAG,SAAS,CAAC;AACtD;MACF;MAEA,KAAK,QAAQ;AACX,YAAI,QAAQ,WAAW,KAAK,QAAQ,CAAC,EAAE,SAAS,QAAQ;AACtD,mBAAS,KAAK;YACZ,MAAM;YACN,SAAS,QAAQ,CAAC,EAAE;YACpB,GAAG,kBAAkB,QAAQ,CAAC,CAAC;UACjC,CAAC;AACD;QACF;AAEA,iBAAS,KAAK;UACZ,MAAM;UACN,SAAS,QAAQ,IAAI,CAAA,SAAQ;AAtCvC,gBAAA;AAuCY,kBAAM,eAAe,kBAAkB,IAAI;AAC3C,oBAAQ,KAAK,MAAM;cACjB,KAAK,QAAQ;AACX,uBAAO,EAAE,MAAM,QAAQ,MAAM,KAAK,MAAM,GAAG,aAAa;cAC1D;cACA,KAAK,SAAS;AACZ,uBAAO;kBACL,MAAM;kBACN,WAAW;oBACT,KACE,KAAK,iBAAiB,MAClB,KAAK,MAAM,SAAS,IACpB,SACE,KAAA,KAAK,aAAL,OAAA,KAAiB,YACnB,WAAW,0BAA0B,KAAK,KAAK,CAAC;kBACxD;kBACA,GAAG;gBACL;cACF;cACA,KAAK,QAAQ;AACX,sBAAM,IAAI,8BAA8B;kBACtC,eAAe;gBACjB,CAAC;cACH;YACF;UACF,CAAC;UACD,GAAG;QACL,CAAC;AAED;MACF;MAEA,KAAK,aAAa;AAChB,YAAI,OAAO;AACX,cAAM,YAID,CAAC;AAEN,mBAAW,QAAQ,SAAS;AAC1B,gBAAM,eAAe,kBAAkB,IAAI;AAC3C,kBAAQ,KAAK,MAAM;YACjB,KAAK,QAAQ;AACX,sBAAQ,KAAK;AACb;YACF;YACA,KAAK,aAAa;AAChB,wBAAU,KAAK;gBACb,IAAI,KAAK;gBACT,MAAM;gBACN,UAAU;kBACR,MAAM,KAAK;kBACX,WAAW,KAAK,UAAU,KAAK,IAAI;gBACrC;gBACA,GAAG;cACL,CAAC;AACD;YACF;UACF;QACF;AAEA,iBAAS,KAAK;UACZ,MAAM;UACN,SAAS;UACT,YAAY,UAAU,SAAS,IAAI,YAAY;UAC/C,GAAG;QACL,CAAC;AAED;MACF;MAEA,KAAK,QAAQ;AACX,mBAAW,gBAAgB,SAAS;AAClC,gBAAM,uBAAuB,kBAAkB,YAAY;AAC3D,mBAAS,KAAK;YACZ,MAAM;YACN,cAAc,aAAa;YAC3B,SAAS,KAAK,UAAU,aAAa,MAAM;YAC3C,GAAG;UACL,CAAC;QACH;AACA;MACF;MAEA,SAAS;AACP,cAAM,mBAA0B;AAChC,cAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;MACzD;IACF;EACF;AAEA,SAAO;AACT;ACpIO,SAAS,oBAAoB;EAClC;EACA;EACA;AACF,GAIG;AACD,SAAO;IACL,IAAI,MAAA,OAAA,KAAM;IACV,SAAS,SAAA,OAAA,QAAS;IAClB,WAAW,WAAW,OAAO,IAAI,KAAK,UAAU,GAAI,IAAI;EAC1D;AACF;ACZO,SAAS,gCACd,cAC6B;AAC7B,UAAQ,cAAc;IACpB,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;IACT,KAAK;AACH,aAAO;IACT,KAAK;IACL,KAAK;AACH,aAAO;IACT;AACE,aAAO;EACX;AACF;AChBO,IAAM,kCAAkC,iBAAE,OAAO;EACtD,OAAO,iBAAE,OAAO;IACd,SAAS,iBAAE,OAAO;;;;IAKlB,MAAM,iBAAE,OAAO,EAAE,QAAQ;IACzB,OAAO,iBAAE,IAAI,EAAE,QAAQ;IACvB,MAAM,iBAAE,MAAM,CAAC,iBAAE,OAAO,GAAG,iBAAE,OAAO,CAAC,CAAC,EAAE,QAAQ;EAClD,CAAC;AACH,CAAC;AAYM,IAAM,wCACX;EACE,aAAa;EACb,gBAAgB,CAAA,SAAQ,KAAK,MAAM;AACrC;ACvBK,SAAS,aAAa;EAC3B;EACA;AACF,GAuBE;AAhCF,MAAA;AAkCE,QAAM,UAAQ,KAAA,KAAK,UAAL,OAAA,SAAA,GAAY,UAAS,KAAK,QAAQ;AAChD,QAAM,eAA6C,CAAC;AAEpD,MAAI,SAAS,MAAM;AACjB,WAAO,EAAE,OAAO,QAAW,aAAa,QAAW,aAAa;EAClE;AAEA,QAAM,aAAa,KAAK;AAExB,QAAM,oBAOD,CAAC;AAEN,aAAW,QAAQ,OAAO;AACxB,QAAI,KAAK,SAAS,oBAAoB;AACpC,mBAAa,KAAK,EAAE,MAAM,oBAAoB,KAAK,CAAC;IACtD,OAAO;AACL,wBAAkB,KAAK;QACrB,MAAM;QACN,UAAU;UACR,MAAM,KAAK;UACX,aAAa,KAAK;UAClB,YAAY,KAAK;QACnB;MACF,CAAC;IACH;EACF;AAEA,MAAI,cAAc,MAAM;AACtB,WAAO,EAAE,OAAO,mBAAmB,aAAa,QAAW,aAAa;EAC1E;AAEA,QAAM,OAAO,WAAW;AAExB,UAAQ,MAAM;IACZ,KAAK;IACL,KAAK;IACL,KAAK;AACH,aAAO,EAAE,OAAO,mBAAmB,aAAa,MAAM,aAAa;IACrE,KAAK;AACH,aAAO;QACL,OAAO;QACP,aAAa;UACX,MAAM;UACN,UAAU;YACR,MAAM,WAAW;UACnB;QACF;QACA;MACF;IACF,SAAS;AACP,YAAM,mBAA0B;AAChC,YAAM,IAAIA,8BAA8B;QACtC,eAAe,iCAAiC,gBAAgB;MAClE,CAAC;IACH;EACF;AACF;ALrCO,IAAM,oCAAN,MAAmE;;EAYxE,YACE,SACA,UACA,QACA;AAfF,SAAS,uBAAuB;AA5DlC,QAAA,IAAA;AA4EI,SAAK,UAAU;AACf,SAAK,WAAW;AAChB,SAAK,SAAS;AAGd,UAAM,kBACJ,KAAA,OAAO,mBAAP,OAAA,KAAyB;AAC3B,SAAK,cAAc;MACjB,eAAe;IACjB;AACA,SAAK,wBAAwB,+BAA+B,cAAc;AAE1E,SAAK,6BAA4B,KAAA,OAAO,8BAAP,OAAA,KAAoC;EACvE;EAEA,IAAI,8BAA2D;AAC7D,WAAO,KAAK,OAAO;EACrB;EAEA,IAAI,WAAmB;AACrB,WAAO,KAAK,OAAO;EACrB;EAEA,IAAY,sBAA8B;AACxC,WAAO,KAAK,OAAO,SAAS,MAAM,GAAG,EAAE,CAAC,EAAE,KAAK;EACjD;EAEQ,QAAQ;IACd;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;EACF,GAAiD;AApHnD,QAAA,IAAA,IAAA,IAAA,IAAA;AAqHI,UAAM,OAAO,KAAK;AAElB,UAAM,WAAyC,CAAC;AAEhD,QAAI,QAAQ,MAAM;AAChB,eAAS,KAAK;QACZ,MAAM;QACN,SAAS;MACX,CAAC;IACH;AAEA,SACE,kBAAA,OAAA,SAAA,eAAgB,UAAS,UACzB,eAAe,UAAU,QACzB,CAAC,KAAK,2BACN;AACA,eAAS,KAAK;QACZ,MAAM;QACN,SAAS;QACT,SACE;MACJ,CAAC;IACH;AAEA,UAAM,WAAW;;MAEf,OAAO,KAAK;;MAGZ,MAAM,KAAK,SAAS;;MAGpB,YAAY;MACZ;MACA,OAAO;MACP,mBAAmB;MACnB,kBAAkB;MAClB,kBACE,kBAAA,OAAA,SAAA,eAAgB,UAAS,SACrB,KAAK,8BAA8B,QACnC,eAAe,UAAU,OACvB;QACE,MAAM;QACN,aAAa;UACX,QAAQ,eAAe;UACvB,OAAM,KAAA,eAAe,SAAf,OAAA,KAAuB;UAC7B,aAAa,eAAe;QAC9B;MACF,IACA,EAAE,MAAM,cAAc,IACxB;MAEN,MAAM;MACN;MACA,GAAG,oBAAA,OAAA,SAAA,iBAAmB,KAAK,mBAAA;MAE3B,mBACE,MAAA,KAAA,oBAAA,OAAA,SAAA,iBAAmB,KAAK,mBAAA,MAAxB,OAAA,SAAA,GAA8C,oBAA9C,OAAA,MACA,KAAA,oBAAA,OAAA,SAAA,iBAAmB,mBAAA,MAAnB,OAAA,SAAA,GAAyC;;MAG3C,UAAU,sCAAsC,MAAM;IACxD;AAEA,YAAQ,MAAM;MACZ,KAAK,WAAW;AACd,cAAM,EAAE,OAAO,aAAa,aAAa,IAAI,aAAa;UACxD;UACA,mBAAmB,KAAK;QAC1B,CAAC;AAED,eAAO;UACL,MAAM,EAAE,GAAG,UAAU,OAAO,YAAY;UACxC,UAAU,CAAC,GAAG,UAAU,GAAG,YAAY;QACzC;MACF;MAEA,KAAK,eAAe;AAClB,eAAO;UACL,MAAM;YACJ,GAAG;YACH,iBACE,KAAK,8BAA8B,QAAQ,KAAK,UAAU,OACtD;cACE,MAAM;cACN,aAAa;gBACX,QAAQ,KAAK;gBACb,OAAM,KAAA,KAAK,SAAL,OAAA,KAAa;gBACnB,aAAa,KAAK;cACpB;YACF,IACA,EAAE,MAAM,cAAc;UAC9B;UACA;QACF;MACF;MAEA,KAAK,eAAe;AAClB,eAAO;UACL,MAAM;YACJ,GAAG;YACH,aAAa;cACX,MAAM;cACN,UAAU,EAAE,MAAM,KAAK,KAAK,KAAK;YACnC;YACA,OAAO;cACL;gBACE,MAAM;gBACN,UAAU;kBACR,MAAM,KAAK,KAAK;kBAChB,aAAa,KAAK,KAAK;kBACvB,YAAY,KAAK,KAAK;gBACxB;cACF;YACF;UACF;UACA;QACF;MACF;MAEA,SAAS;AACP,cAAM,mBAA0B;AAChC,cAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;MACzD;IACF;EACF;EAEA,MAAM,WACJ,SAC6D;AAtPjE,QAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;AAuPI,UAAM,EAAE,MAAM,SAAS,IAAI,KAAK,QAAQ,EAAE,GAAG,QAAQ,CAAC;AAEtD,UAAM,OAAO,KAAK,UAAU,IAAI;AAEhC,UAAM;MACJ;MACA,OAAO;MACP,UAAU;IACZ,IAAI,MAAM,cAAc;MACtB,KAAK,KAAK,OAAO,IAAI;QACnB,MAAM;QACN,SAAS,KAAK;MAChB,CAAC;MACD,SAAS,eAAe,KAAK,OAAO,QAAQ,GAAG,QAAQ,OAAO;MAC9D,MAAM;MACN,uBAAuB,KAAK;MAC5B,2BAA2B;QACzB;MACF;MACA,aAAa,QAAQ;MACrB,OAAO,KAAK,OAAO;IACrB,CAAC;AAED,UAAM,EAAE,UAAU,WAAW,GAAG,YAAY,IAAI;AAChD,UAAM,SAAS,aAAa,QAAQ,CAAC;AAGrC,UAAM,mBAAoD;MACxD,CAAC,KAAK,mBAAmB,GAAG,CAAC;MAC7B,IAAG,MAAA,KAAA,KAAK,OAAO,sBAAZ,OAAA,SAAA,GAA+B,oBAA/B,OAAA,SAAA,GAAA,KAAA,IAAiD;QAClD,YAAY;MACd,CAAA;IACF;AACA,UAAM,0BACJ,KAAA,aAAa,UAAb,OAAA,SAAA,GAAoB;AACtB,UAAM,sBAAqB,KAAA,aAAa,UAAb,OAAA,SAAA,GAAoB;AAC/C,SAAI,0BAAA,OAAA,SAAA,uBAAwB,qBAAoB,MAAM;AACpD,uBAAiB,KAAK,mBAAmB,EAAE,kBACzC,0BAAA,OAAA,SAAA,uBAAwB;IAC5B;AACA,SAAI,0BAAA,OAAA,SAAA,uBAAwB,+BAA8B,MAAM;AAC9D,uBAAiB,KAAK,mBAAmB,EAAE,2BACzC,0BAAA,OAAA,SAAA,uBAAwB;IAC5B;AACA,SAAI,0BAAA,OAAA,SAAA,uBAAwB,+BAA8B,MAAM;AAC9D,uBAAiB,KAAK,mBAAmB,EAAE,2BACzC,0BAAA,OAAA,SAAA,uBAAwB;IAC5B;AACA,SAAI,sBAAA,OAAA,SAAA,mBAAoB,kBAAiB,MAAM;AAC7C,uBAAiB,KAAK,mBAAmB,EAAE,qBACzC,sBAAA,OAAA,SAAA,mBAAoB;IACxB;AAEA,WAAO;MACL,OAAM,KAAA,OAAO,QAAQ,YAAf,OAAA,KAA0B;MAChC,YAAW,KAAA,OAAO,QAAQ,sBAAf,OAAA,KAAoC;MAC/C,YAAW,KAAA,OAAO,QAAQ,eAAf,OAAA,SAAA,GAA2B,IAAI,CAAA,aAAS;AA/SzD,YAAAC;AA+S6D,eAAA;UACrD,cAAc;UACd,aAAYA,MAAA,SAAS,OAAT,OAAAA,MAAe,WAAW;UACtC,UAAU,SAAS,SAAS;UAC5B,MAAM,SAAS,SAAS;QAC1B;MAAA,CAAA;MACA,cAAc,gCAAgC,OAAO,aAAa;MAClE,OAAO;QACL,eAAc,MAAA,KAAA,aAAa,UAAb,OAAA,SAAA,GAAoB,kBAApB,OAAA,KAAqC;QACnD,mBAAkB,MAAA,KAAA,aAAa,UAAb,OAAA,SAAA,GAAoB,sBAApB,OAAA,KAAyC;MAC7D;MACA;MACA,SAAS,EAAE,WAAW,YAAY;MAClC,aAAa,EAAE,SAAS,iBAAiB,MAAM,YAAY;MAC3D,UAAU,oBAAoB,YAAY;MAC1C;MACA,SAAS,EAAE,KAAK;IAClB;EACF;EAEA,MAAM,SACJ,SAC2D;AArU/D,QAAA;AAsUI,QAAI,KAAK,SAAS,mBAAmB;AACnC,YAAM,SAAS,MAAM,KAAK,WAAW,OAAO;AAC5C,YAAM,kBAAkB,IAAI,eAA0C;QACpE,MAAM,YAAY;AAChB,qBAAW,QAAQ,EAAE,MAAM,qBAAqB,GAAG,OAAO,SAAS,CAAC;AACpE,cAAI,OAAO,WAAW;AACpB,gBAAI,MAAM,QAAQ,OAAO,SAAS,GAAG;AACnC,yBAAW,QAAQ,OAAO,WAAW;AACnC,oBAAI,KAAK,SAAS,QAAQ;AACxB,6BAAW,QAAQ;oBACjB,MAAM;oBACN,WAAW,KAAK;kBAClB,CAAC;gBACH;cACF;YACF,OAAO;AACL,yBAAW,QAAQ;gBACjB,MAAM;gBACN,WAAW,OAAO;cACpB,CAAC;YACH;UACF;AACA,cAAI,OAAO,MAAM;AACf,uBAAW,QAAQ;cACjB,MAAM;cACN,WAAW,OAAO;YACpB,CAAC;UACH;AACA,cAAI,OAAO,WAAW;AACpB,uBAAW,YAAY,OAAO,WAAW;AACvC,yBAAW,QAAQ;gBACjB,MAAM;gBACN,GAAG;cACL,CAAC;YACH;UACF;AACA,qBAAW,QAAQ;YACjB,MAAM;YACN,cAAc,OAAO;YACrB,OAAO,OAAO;YACd,UAAU,OAAO;YACjB,kBAAkB,OAAO;UAC3B,CAAC;AACD,qBAAW,MAAM;QACnB;MACF,CAAC;AACD,aAAO;QACL,QAAQ;QACR,SAAS,OAAO;QAChB,aAAa,OAAO;QACpB,UAAU,OAAO;MACnB;IACF;AAEA,UAAM,EAAE,MAAM,SAAS,IAAI,KAAK,QAAQ,EAAE,GAAG,QAAQ,CAAC;AAEtD,UAAM,OAAO;MACX,GAAG;MACH,QAAQ;;MAGR,gBAAgB,KAAK,OAAO,eACxB,EAAE,eAAe,KAAK,IACtB;IACN;AAEA,UAAM,qBACJ,KAAA,KAAK,OAAO,sBAAZ,OAAA,SAAA,GAA+B,sBAAA;AAEjC,UAAM,EAAE,iBAAiB,OAAO,SAAS,IAAI,MAAM,cAAc;MAC/D,KAAK,KAAK,OAAO,IAAI;QACnB,MAAM;QACN,SAAS,KAAK;MAChB,CAAC;MACD,SAAS,eAAe,KAAK,OAAO,QAAQ,GAAG,QAAQ,OAAO;MAC9D;MACA,uBAAuB,KAAK;MAC5B,2BAA2B;QACzB,KAAK;MACP;MACA,aAAa,QAAQ;MACrB,OAAO,KAAK,OAAO;IACrB,CAAC;AAED,UAAM,EAAE,UAAU,WAAW,GAAG,YAAY,IAAI;AAEhD,UAAM,YAQD,CAAC;AAEN,QAAI,eAA4C;AAChD,QAAI,QAWA;MACF,kBAAkB;MAClB,yBAAyB;QACvB,iBAAiB;QACjB,0BAA0B;QAC1B,0BAA0B;MAC5B;MACA,cAAc;MACd,qBAAqB;QACnB,cAAc;MAChB;IACF;AACA,QAAI,eAAe;AACnB,QAAI,sBAAsB,KAAK;AAE/B,WAAO;MACL,QAAQ,SAAS;QACf,IAAI,gBAGF;;UAEA,UAAU,OAAO,YAAY;AAxcvC,gBAAAA,KAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;AA0cY,gBAAI,CAAC,MAAM,SAAS;AAClB,6BAAe;AACf,yBAAW,QAAQ,EAAE,MAAM,SAAS,OAAO,MAAM,MAAM,CAAC;AACxD;YACF;AACA,kBAAM,QAAQ,MAAM;AAEpB,iCAAA,OAAA,SAAA,kBAAmB,aAAa,MAAM,QAAA;AAGtC,gBAAI,WAAW,OAAO;AACpB,6BAAe;AACf,yBAAW,QAAQ,EAAE,MAAM,SAAS,OAAO,MAAM,MAAM,QAAQ,CAAC;AAChE;YACF;AAEA,gBAAI,cAAc;AAChB,6BAAe;AAEf,yBAAW,QAAQ;gBACjB,MAAM;gBACN,GAAG,oBAAoB,KAAK;cAC9B,CAAC;YACH;AAEA,gBAAI,MAAM,SAAS,MAAM;AACvB,oBAAM;gBACJ;gBACA;gBACA;gBACA;cACF,IAAI,MAAM;AAEV,oBAAM,eAAe,iBAAA,OAAA,gBAAiB;AACtC,oBAAM,mBAAmB,qBAAA,OAAA,oBAAqB;AAE9C,mBAAI,6BAAA,OAAA,SAAA,0BAA2B,qBAAoB,MAAM;AACvD,sBAAM,wBAAwB,kBAC5B,6BAAA,OAAA,SAAA,0BAA2B;cAC/B;AACA,mBACE,6BAAA,OAAA,SAAA,0BAA2B,+BAA8B,MACzD;AACA,sBAAM,wBAAwB,2BAC5B,6BAAA,OAAA,SAAA,0BAA2B;cAC/B;AACA,mBACE,6BAAA,OAAA,SAAA,0BAA2B,+BAA8B,MACzD;AACA,sBAAM,wBAAwB,2BAC5B,6BAAA,OAAA,SAAA,0BAA2B;cAC/B;AACA,mBAAI,yBAAA,OAAA,SAAA,sBAAuB,kBAAiB,MAAM;AAChD,sBAAM,oBAAoB,eACxB,yBAAA,OAAA,SAAA,sBAAuB;cAC3B;YACF;AAEA,kBAAM,SAAS,MAAM,QAAQ,CAAC;AAE9B,iBAAI,UAAA,OAAA,SAAA,OAAQ,kBAAiB,MAAM;AACjC,6BAAe;gBACb,OAAO;cACT;YACF;AAEA,iBAAI,UAAA,OAAA,SAAA,OAAQ,UAAS,MAAM;AACzB;YACF;AAEA,kBAAM,QAAQ,OAAO;AAGrB,gBAAI,MAAM,qBAAqB,MAAM;AACnC,yBAAW,QAAQ;gBACjB,MAAM;gBACN,WAAW,MAAM;cACnB,CAAC;YACH;AAEA,gBAAI,MAAM,WAAW,MAAM;AACzB,yBAAW,QAAQ;gBACjB,MAAM;gBACN,WAAW,MAAM;cACnB,CAAC;YACH;AAEA,gBAAI,MAAM,cAAc,MAAM;AAC5B,yBAAW,iBAAiB,MAAM,YAAY;AAC5C,sBAAM,QAAQ,cAAc;AAE5B,oBAAI,UAAU,KAAK,KAAK,MAAM;AAC5B,sBAAI,cAAc,SAAS,YAAY;AACrC,0BAAM,IAAI,yBAAyB;sBACjC,MAAM;sBACN,SAAS;oBACX,CAAC;kBACH;AAEA,sBAAI,cAAc,MAAM,MAAM;AAC5B,0BAAM,IAAI,yBAAyB;sBACjC,MAAM;sBACN,SAAS;oBACX,CAAC;kBACH;AAEA,wBAAIA,MAAA,cAAc,aAAd,OAAA,SAAAA,IAAwB,SAAQ,MAAM;AACxC,0BAAM,IAAI,yBAAyB;sBACjC,MAAM;sBACN,SAAS;oBACX,CAAC;kBACH;AAEA,4BAAU,KAAK,IAAI;oBACjB,IAAI,cAAc;oBAClB,MAAM;oBACN,UAAU;sBACR,MAAM,cAAc,SAAS;sBAC7B,YAAW,KAAA,cAAc,SAAS,cAAvB,OAAA,KAAoC;oBACjD;oBACA,aAAa;kBACf;AAEA,wBAAMC,YAAW,UAAU,KAAK;AAEhC,wBACE,KAAAA,UAAS,aAAT,OAAA,SAAA,GAAmB,SAAQ,UAC3B,KAAAA,UAAS,aAAT,OAAA,SAAA,GAAmB,cAAa,MAChC;AAEA,wBAAIA,UAAS,SAAS,UAAU,SAAS,GAAG;AAC1C,iCAAW,QAAQ;wBACjB,MAAM;wBACN,cAAc;wBACd,YAAYA,UAAS;wBACrB,UAAUA,UAAS,SAAS;wBAC5B,eAAeA,UAAS,SAAS;sBACnC,CAAC;oBACH;AAIA,wBAAI,eAAeA,UAAS,SAAS,SAAS,GAAG;AAC/C,iCAAW,QAAQ;wBACjB,MAAM;wBACN,cAAc;wBACd,aAAY,KAAAA,UAAS,OAAT,OAAA,KAAe,WAAW;wBACtC,UAAUA,UAAS,SAAS;wBAC5B,MAAMA,UAAS,SAAS;sBAC1B,CAAC;AACDA,gCAAS,cAAc;oBACzB;kBACF;AAEA;gBACF;AAGA,sBAAM,WAAW,UAAU,KAAK;AAEhC,oBAAI,SAAS,aAAa;AACxB;gBACF;AAEA,sBAAI,KAAA,cAAc,aAAd,OAAA,SAAA,GAAwB,cAAa,MAAM;AAC7C,2BAAS,SAAU,cACjB,MAAA,KAAA,cAAc,aAAd,OAAA,SAAA,GAAwB,cAAxB,OAAA,KAAqC;gBACzC;AAGA,2BAAW,QAAQ;kBACjB,MAAM;kBACN,cAAc;kBACd,YAAY,SAAS;kBACrB,UAAU,SAAS,SAAS;kBAC5B,gBAAe,KAAA,cAAc,SAAS,cAAvB,OAAA,KAAoC;gBACrD,CAAC;AAGD,sBACE,KAAA,SAAS,aAAT,OAAA,SAAA,GAAmB,SAAQ,UAC3B,KAAA,SAAS,aAAT,OAAA,SAAA,GAAmB,cAAa,QAChC,eAAe,SAAS,SAAS,SAAS,GAC1C;AACA,6BAAW,QAAQ;oBACjB,MAAM;oBACN,cAAc;oBACd,aAAY,KAAA,SAAS,OAAT,OAAA,KAAe,WAAW;oBACtC,UAAU,SAAS,SAAS;oBAC5B,MAAM,SAAS,SAAS;kBAC1B,CAAC;AACD,2BAAS,cAAc;gBACzB;cACF;YACF;UACF;UAEA,MAAM,YAAY;AA/oB5B,gBAAAD,KAAA;AAgpBY,kBAAM,mBAAoD;cACxD,CAAC,mBAAmB,GAAG,CAAC;cACxB,GAAG,qBAAA,OAAA,SAAA,kBAAmB,cAAA;YACxB;AACA,gBAAI,MAAM,wBAAwB,mBAAmB,MAAM;AACzD,+BAAiB,mBAAmB,EAAE,kBACpC,MAAM,wBAAwB;YAClC;AACA,gBACE,MAAM,wBAAwB,4BAA4B,MAC1D;AACA,+BAAiB,mBAAmB,EAAE,2BACpC,MAAM,wBAAwB;YAClC;AACA,gBACE,MAAM,wBAAwB,4BAA4B,MAC1D;AACA,+BAAiB,mBAAmB,EAAE,2BACpC,MAAM,wBAAwB;YAClC;AACA,gBAAI,MAAM,oBAAoB,gBAAgB,MAAM;AAClD,+BAAiB,mBAAmB,EAAE,qBACpC,MAAM,oBAAoB;YAC9B;AAEA,uBAAW,QAAQ;cACjB,MAAM;cACN;cACA,OAAO;gBACL,eAAcA,MAAA,MAAM,iBAAN,OAAAA,MAAsB;gBACpC,mBAAkB,KAAA,MAAM,qBAAN,OAAA,KAA0B;cAC9C;cACA;YACF,CAAC;UACH;QACF,CAAC;MACH;MACA,SAAS,EAAE,WAAW,YAAY;MAClC,aAAa,EAAE,SAAS,gBAAgB;MACxC;MACA,SAAS,EAAE,MAAM,KAAK,UAAU,IAAI,EAAE;IACxC;EACF;AACF;AAEA,IAAM,mCAAmCE,iBACtC,OAAO;EACN,eAAeA,iBAAE,OAAO,EAAE,QAAQ;EAClC,mBAAmBA,iBAAE,OAAO,EAAE,QAAQ;EACtC,uBAAuBA,iBACpB,OAAO;IACN,eAAeA,iBAAE,OAAO,EAAE,QAAQ;EACpC,CAAC,EACA,QAAQ;EACX,2BAA2BA,iBACxB,OAAO;IACN,kBAAkBA,iBAAE,OAAO,EAAE,QAAQ;IACrC,4BAA4BA,iBAAE,OAAO,EAAE,QAAQ;IAC/C,4BAA4BA,iBAAE,OAAO,EAAE,QAAQ;EACjD,CAAC,EACA,QAAQ;AACb,CAAC,EACA,QAAQ;AAIX,IAAM,qCAAqCA,iBAAE,OAAO;EAClD,IAAIA,iBAAE,OAAO,EAAE,QAAQ;EACvB,SAASA,iBAAE,OAAO,EAAE,QAAQ;EAC5B,OAAOA,iBAAE,OAAO,EAAE,QAAQ;EAC1B,SAASA,iBAAE;IACTA,iBAAE,OAAO;MACP,SAASA,iBAAE,OAAO;QAChB,MAAMA,iBAAE,QAAQ,WAAW,EAAE,QAAQ;QACrC,SAASA,iBAAE,OAAO,EAAE,QAAQ;QAC5B,mBAAmBA,iBAAE,OAAO,EAAE,QAAQ;QACtC,YAAYA,iBACT;UACCA,iBAAE,OAAO;YACP,IAAIA,iBAAE,OAAO,EAAE,QAAQ;YACvB,MAAMA,iBAAE,QAAQ,UAAU;YAC1B,UAAUA,iBAAE,OAAO;cACjB,MAAMA,iBAAE,OAAO;cACf,WAAWA,iBAAE,OAAO;YACtB,CAAC;UACH,CAAC;QACH,EACC,QAAQ;MACb,CAAC;MACD,eAAeA,iBAAE,OAAO,EAAE,QAAQ;IACpC,CAAC;EACH;EACA,OAAO;AACT,CAAC;AAID,IAAM,wCAAwC,CAC5C,gBAEAA,iBAAE,MAAM;EACNA,iBAAE,OAAO;IACP,IAAIA,iBAAE,OAAO,EAAE,QAAQ;IACvB,SAASA,iBAAE,OAAO,EAAE,QAAQ;IAC5B,OAAOA,iBAAE,OAAO,EAAE,QAAQ;IAC1B,SAASA,iBAAE;MACTA,iBAAE,OAAO;QACP,OAAOA,iBACJ,OAAO;UACN,MAAMA,iBAAE,KAAK,CAAC,WAAW,CAAC,EAAE,QAAQ;UACpC,SAASA,iBAAE,OAAO,EAAE,QAAQ;UAC5B,mBAAmBA,iBAAE,OAAO,EAAE,QAAQ;UACtC,YAAYA,iBACT;YACCA,iBAAE,OAAO;cACP,OAAOA,iBAAE,OAAO,EAAE,SAAS;cAC3B,IAAIA,iBAAE,OAAO,EAAE,QAAQ;cACvB,MAAMA,iBAAE,QAAQ,UAAU,EAAE,QAAQ;cACpC,UAAUA,iBAAE,OAAO;gBACjB,MAAMA,iBAAE,OAAO,EAAE,QAAQ;gBACzB,WAAWA,iBAAE,OAAO,EAAE,QAAQ;cAChC,CAAC;YACH,CAAC;UACH,EACC,QAAQ;QACb,CAAC,EACA,QAAQ;QACX,eAAeA,iBAAE,OAAO,EAAE,QAAQ;MACpC,CAAC;IACH;IACA,OAAO;EACT,CAAC;EACD;AACF,CAAC;AMtcH,IAAM,2CAA2CC,iBAAE,OAAO;EACxD,IAAIA,iBAAE,OAAO,EAAE,QAAQ;EACvB,SAASA,iBAAE,OAAO,EAAE,QAAQ;EAC5B,OAAOA,iBAAE,OAAO,EAAE,QAAQ;EAC1B,SAASA,iBAAE;IACTA,iBAAE,OAAO;MACP,MAAMA,iBAAE,OAAO;MACf,eAAeA,iBAAE,OAAO;IAC1B,CAAC;EACH;EACA,OAAOA,iBACJ,OAAO;IACN,eAAeA,iBAAE,OAAO;IACxB,mBAAmBA,iBAAE,OAAO;EAC9B,CAAC,EACA,QAAQ;AACb,CAAC;AEtOD,IAAM,oCAAoCC,iBAAE,OAAO;EACjD,MAAMA,iBAAE,MAAMA,iBAAE,OAAO,EAAE,WAAWA,iBAAE,MAAMA,iBAAE,OAAO,CAAC,EAAE,CAAC,CAAC;EAC1D,OAAOA,iBAAE,OAAO,EAAE,eAAeA,iBAAE,OAAO,EAAE,CAAC,EAAE,QAAQ;AACzD,CAAC;ACjGM,IAAM,6BAAN,MAAyD;EAW9D,YACW,SACQ,UACA,QACjB;AAHS,SAAA,UAAA;AACQ,SAAA,WAAA;AACA,SAAA,SAAA;AAbnB,SAAS,uBAAuB;EAc7B;EAZH,IAAI,mBAA2B;AA9BjC,QAAA;AA+BI,YAAO,KAAA,KAAK,SAAS,qBAAd,OAAA,KAAkC;EAC3C;EAEA,IAAI,WAAmB;AACrB,WAAO,KAAK,OAAO;EACrB;EAQA,MAAM,WAAW;IACf;IACA;IACA;IACA;IACA;IACA;IACA;IACA;EACF,GAEE;AAvDJ,QAAA,IAAA,IAAA,IAAA,IAAA;AAwDI,UAAM,WAA2C,CAAC;AAElD,QAAI,eAAe,MAAM;AACvB,eAAS,KAAK;QACZ,MAAM;QACN,SAAS;QACT,SACE;MACJ,CAAC;IACH;AAEA,QAAI,QAAQ,MAAM;AAChB,eAAS,KAAK,EAAE,MAAM,uBAAuB,SAAS,OAAO,CAAC;IAChE;AAEA,UAAM,eAAc,MAAA,MAAA,KAAA,KAAK,OAAO,cAAZ,OAAA,SAAA,GAAuB,gBAAvB,OAAA,SAAA,GAAA,KAAA,EAAA,MAAA,OAAA,KAA0C,oBAAI,KAAK;AACvE,UAAM,EAAE,OAAO,UAAU,gBAAgB,IAAI,MAAMC,cAAc;MAC/D,KAAK,KAAK,OAAO,IAAI;QACnB,MAAM;QACN,SAAS,KAAK;MAChB,CAAC;MACD,SAASC,eAAe,KAAK,OAAO,QAAQ,GAAG,OAAO;MACtD,MAAM;QACJ,OAAO,KAAK;QACZ;QACA;QACA;QACA,IAAI,KAAA,gBAAgB,WAAhB,OAAA,KAA0B,CAAC;QAC/B,iBAAiB;QACjB,GAAI,KAAK,SAAS,OAAO,EAAE,MAAM,KAAK,SAAS,KAAK,IAAI,CAAC;MAC3D;MACA,uBAAuBC;SACrB,KAAA,KAAK,OAAO,mBAAZ,OAAA,KAA8B;MAChC;MACA,2BAA2BC;QACzB;MACF;MACA;MACA,OAAO,KAAK,OAAO;IACrB,CAAC;AAED,WAAO;MACL,QAAQ,SAAS,KAAK,IAAI,CAAA,SAAQ,KAAK,QAAQ;MAC/C;MACA,UAAU;QACR,WAAW;QACX,SAAS,KAAK;QACd,SAAS;MACX;IACF;EACF;AACF;AAIA,IAAM,sCAAsCJ,iBAAE,OAAO;EACnD,MAAMA,iBAAE,MAAMA,iBAAE,OAAO,EAAE,UAAUA,iBAAE,OAAO,EAAE,CAAC,CAAC;AAClD,CAAC;;;AGnFM,SAAS,0BAA0B,SAAyB;AACjE,SAAO;IACL;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;EACF,EAAE,SAAS,OAAO;AACpB;AC5CO,IAAM,iBAAiB,iBAAE,OAAO;EACrC,MAAM,iBAAE,OAAO;EACf,OAAO,iBAAE,OAAO;AAClB,CAAC;AFmBD,IAAM,oBAA0D;EAC9D,aAAa;EACb,gBAAgB,CAAA,SAAQ,KAAK;AAC/B;AA6DO,SAAS,UAAU,UAA+B,CAAC,GAAgB;AAzF1E,MAAA;AA0FE,QAAM,UAAU;KACd,KAAA,QAAQ,YAAR,OAAA,KAAmB;EACrB;AACA,QAAM,aAAa,OAAO;IACxB,eAAe,UAAU,WAAW;MAClC,QAAQ,QAAQ;MAChB,yBAAyB;MACzB,aAAa;IACf,CAAC,CAAC;IACF,GAAG,QAAQ;EACb;AAEA,QAAM,sBAAsB,CAC1B,SACA,WAA4B,CAAC,MAC1B;AACH,UAAM,oBAAoB,0BAA0B,OAAO;AAC3D,WAAO,IAAI,kCAAkC,SAAS,UAAU;MAC9D,UAAU;MACV,KAAK,CAAC,EAAE,KAAK,MAAM,GAAG,OAAO,GAAG,IAAI;MACpC,SAAS;MACT,OAAO,QAAQ;MACf,6BAA6B,oBAAoB,SAAS;MAC1D,gBAAgB;MAChB,2BAA2B;MAC3B,cAAc;IAChB,CAAC;EACH;AAEA,QAAM,mBAAmB,CACvB,SACA,WAA6B,CAAC,MAC3B;AACH,WAAO,IAAI,2BAA2B,SAAS,UAAU;MACvD,UAAU;MACV,KAAK,CAAC,EAAE,KAAK,MAAM,GAAG,OAAO,GAAG,IAAI;MACpC,SAAS;MACT,OAAO,QAAQ;MACf,gBAAgB;IAClB,CAAC;EACH;AAEA,QAAM,WAAW,CAAC,SAAyB,aACzC,oBAAoB,SAAS,QAAQ;AAEvC,WAAS,gBAAgB;AACzB,WAAS,OAAO;AAChB,WAAS,qBAAqB,CAAC,YAAoB;AACjD,UAAM,IAAI,iBAAiB,EAAE,SAAS,WAAW,qBAAqB,CAAC;EACzE;AACA,WAAS,aAAa;AACtB,WAAS,QAAQ;AAEjB,SAAO;AACT;AAEO,IAAM,MAAM,UAAU;",
  "names": ["UnsupportedFunctionalityError", "_a", "toolCall", "z", "z", "z", "postJsonToApi", "combineHeaders", "createJsonErrorResponseHandler", "createJsonResponseHandler"]
}
