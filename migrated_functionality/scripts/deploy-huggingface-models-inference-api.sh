#!/bin/bash

# IZA OS Hugging Face Model Stack Deployment Script - Inference API Version
# Production-grade deployment for 1,842-agent Maestro orchestration using HF Inference API

set -e

echo "ðŸš€ IZA OS Hugging Face Model Stack Deployment - Inference API Version"
echo "===================================================================="
echo "Ecosystem Value: $1.4B+"
echo "Revenue Pipeline: $10M+"
echo "Agent Count: 1,842"
echo "Automation Level: 95%"
echo "Hardware: Apple M4 Max (32 GPU cores)"
echo "Mode: Hugging Face Inference API"
echo ""

# Configuration
LOGS_DIR="./logs"
DATA_DIR="./data"
MONITORING_DIR="./monitoring"
NGINX_DIR="./nginx"

# Create directories
echo "ðŸ“ Creating directories..."
mkdir -p $LOGS_DIR
mkdir -p $DATA_DIR/{redis,postgres}
mkdir -p $MONITORING_DIR/{prometheus,grafana}
mkdir -p $NGINX_DIR/{ssl}

# Check Apple Silicon GPU availability
echo "ðŸ” Checking Apple Silicon GPU availability..."
if command -v system_profiler &> /dev/null; then
    GPU_INFO=$(system_profiler SPDisplaysDataType | grep -A 5 "Apple M4")
    echo "âœ… Apple M4 Max GPU detected:"
    echo "$GPU_INFO"
else
    echo "âŒ Apple Silicon GPU not detected. Please ensure you're running on Apple Silicon."
    exit 1
fi

# Check Docker and Docker Compose
echo "ðŸ³ Checking Docker..."
if command -v docker &> /dev/null; then
    docker --version
    echo "âœ… Docker is installed"
else
    echo "âŒ Docker not found. Please install Docker Desktop for Mac first."
    exit 1
fi

if command -v docker-compose &> /dev/null; then
    docker-compose --version
    echo "âœ… Docker Compose is installed"
else
    echo "âŒ Docker Compose not found. Please install Docker Compose first."
    exit 1
fi

# Check for Hugging Face API Key
echo "ðŸ”‘ Checking Hugging Face API configuration..."
if [ -z "$HUGGINGFACE_API_KEY" ]; then
    echo "âš ï¸  HUGGINGFACE_API_KEY not set. Using public inference endpoints."
    echo "   For better performance, set your HF API key:"
    echo "   export HUGGINGFACE_API_KEY='your_api_key_here'"
    HF_API_KEY=""
else
    echo "âœ… Hugging Face API key configured"
    HF_API_KEY="$HUGGINGFACE_API_KEY"
fi

# Create Inference API based Docker Compose
echo "ðŸŒ Creating Inference API based Docker Compose configuration..."
cat > docker-compose-inference-api.yml << EOF
version: '3.8'

services:
  # IZA OS Model Stack API Gateway
  iza-model-gateway:
    image: python:3.11-slim
    container_name: iza-os-model-gateway
    platform: linux/arm64
    ports:
      - "8080:8080"
    environment:
      - HUGGINGFACE_API_KEY=$HF_API_KEY
      - PORT=8080
    volumes:
      - ./memu/iza_huggingface_model_stack.py:/app/model_stack.py
      - ./logs:/app/logs
    working_dir: /app
    command: >
      bash -c "
        pip install fastapi uvicorn aiohttp python-dotenv &&
        python -c \"
import asyncio
import aiohttp
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os
from typing import Optional, Dict, Any
import json
from datetime import datetime

app = FastAPI(title='IZA OS Hugging Face Model Stack', version='1.0.0')

class ModelRequest(BaseModel):
    prompt: str
    model_type: str = 'ceo'
    max_tokens: Optional[int] = 1000
    temperature: Optional[float] = 0.7
    context: Optional[Dict[str, Any]] = {}

class ModelResponse(BaseModel):
    content: str
    model_name: str
    execution_time: float
    tokens_used: int
    cost: float
    success: bool
    error: Optional[str] = None

# Model configurations
MODELS = {
    'ceo': {
        'name': 'Mixtral-8x7B-Instruct-v0.1',
        'endpoint': 'https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1',
        'cost_per_token': 0.0005
    },
    'cto': {
        'name': 'DeepSeek-Coder-33B-Instruct',
        'endpoint': 'https://api-inference.huggingface.co/models/deepseek-ai/deepseek-coder-33b-instruct',
        'cost_per_token': 0.0008
    },
    'designer': {
        'name': 'LLaVA-1.5-7B',
        'endpoint': 'https://api-inference.huggingface.co/models/liuhaotian/llava-1.5-7b',
        'cost_per_token': 0.0003
    },
    'cfo': {
        'name': 'NuminaMath-7B-TG',
        'endpoint': 'https://api-inference.huggingface.co/models/NuminaAI/NuminaMath-7B-TG',
        'cost_per_token': 0.0002
    },
    'healthcare': {
        'name': 'BioMedLM-2.7B',
        'endpoint': 'https://api-inference.huggingface.co/models/stanford-crfm/BioMedLM',
        'cost_per_token': 0.0001
    },
    'multimedia': {
        'name': 'Fuyu-8B',
        'endpoint': 'https://api-inference.huggingface.co/models/adept/fuyu-8b',
        'cost_per_token': 0.0002
    },
    'global': {
        'name': 'BloomZ-7B1-MT',
        'endpoint': 'https://api-inference.huggingface.co/models/bigscience/bloomz-7b1-mt',
        'cost_per_token': 0.0003
    },
    'micro': {
        'name': 'Phi-2',
        'endpoint': 'https://api-inference.huggingface.co/models/microsoft/phi-2',
        'cost_per_token': 0.00005
    },
    'memory': {
        'name': 'BGE-Large-EN-v1.5',
        'endpoint': 'https://api-inference.huggingface.co/models/BAAI/bge-large-en-v1.5',
        'cost_per_token': 0.0001
    },
    'maestro': {
        'name': 'Zephyr-7B-Beta',
        'endpoint': 'https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta',
        'cost_per_token': 0.0002
    }
}

# IZA OS Business Context
IZA_CONTEXT = {
    'ecosystem_value': '$1.4B+',
    'revenue_pipeline': '$10M+',
    'automation_level': '95%',
    'team_efficiency': '98.7%',
    'agent_count': 1842,
    'success_rate': 0.987,
    'market_position': 'Leading',
    'strategic_focus': 'Autonomous Venture Studio Operations'
}

async def enhance_prompt_with_iza_context(prompt: str, context: Dict[str, Any]) -> str:
    enhanced_context = context.copy()
    enhanced_context.update(IZA_CONTEXT)
    
    iza_context_str = f\"\"\"
IZA OS Autonomous Venture Studio Context:
- Ecosystem Value: {IZA_CONTEXT['ecosystem_value']}
- Revenue Pipeline: {IZA_CONTEXT['revenue_pipeline']}
- Automation Level: {IZA_CONTEXT['automation_level']}
- Team Efficiency: {IZA_CONTEXT['team_efficiency']}
- Agent Count: {IZA_CONTEXT['agent_count']}
- Success Rate: {IZA_CONTEXT['success_rate']:.1%}
- Market Position: {IZA_CONTEXT['market_position']}
- Strategic Focus: {IZA_CONTEXT['strategic_focus']}

Additional Context: {json.dumps(enhanced_context, indent=2)}

User Request: {prompt}
\"\"\"
    return iza_context_str.strip()

async def generate_with_model(model_type: str, prompt: str, max_tokens: int, temperature: float, context: Dict[str, Any]) -> ModelResponse:
    if model_type not in MODELS:
        return ModelResponse(
            content='',
            model_name='unknown',
            execution_time=0,
            tokens_used=0,
            cost=0,
            success=False,
            error=f'Model type {model_type} not found'
        )
    
    model_config = MODELS[model_type]
    start_time = datetime.now()
    
    try:
        # Enhance prompt with IZA OS context
        enhanced_prompt = await enhance_prompt_with_iza_context(prompt, context)
        
        # Prepare request payload
        payload = {
            'inputs': enhanced_prompt,
            'parameters': {
                'max_new_tokens': max_tokens,
                'temperature': temperature,
                'top_p': 0.9,
                'do_sample': True,
                'return_full_text': False
            }
        }
        
        headers = {
            'Content-Type': 'application/json'
        }
        
        if os.getenv('HUGGINGFACE_API_KEY'):
            headers['Authorization'] = f'Bearer {os.getenv(\"HUGGINGFACE_API_KEY\")}'
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                model_config['endpoint'],
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=300)
            ) as response:
                
                if response.status == 200:
                    data = await response.json()
                    
                    # Extract content from response
                    content = ''
                    if isinstance(data, list) and len(data) > 0:
                        content = data[0].get('generated_text', '')
                    elif isinstance(data, dict):
                        content = data.get('generated_text', data.get('text', ''))
                    
                    execution_time = (datetime.now() - start_time).total_seconds()
                    
                    # Calculate tokens and cost
                    tokens_used = len(content.split()) * 1.3  # Rough estimation
                    cost = tokens_used * model_config['cost_per_token']
                    
                    return ModelResponse(
                        content=content.strip(),
                        model_name=model_config['name'],
                        execution_time=execution_time,
                        tokens_used=int(tokens_used),
                        cost=cost,
                        success=True
                    )
                else:
                    error_text = await response.text()
                    execution_time = (datetime.now() - start_time).total_seconds()
                    
                    return ModelResponse(
                        content='',
                        model_name=model_config['name'],
                        execution_time=execution_time,
                        tokens_used=0,
                        cost=0,
                        success=False,
                        error=f'API Error: {response.status} - {error_text}'
                    )
                    
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        return ModelResponse(
            content='',
            model_name=model_config['name'],
            execution_time=execution_time,
            tokens_used=0,
            cost=0,
            success=False,
            error=str(e)
        )

@app.get('/')
async def root():
    return {
        'message': 'IZA OS Hugging Face Model Stack',
        'version': '1.0.0',
        'ecosystem_value': '$1.4B+',
        'revenue_pipeline': '$10M+',
        'agent_count': 1842,
        'automation_level': '95%',
        'team_efficiency': '98.7%',
        'available_models': list(MODELS.keys())
    }

@app.get('/health')
async def health():
    return {'status': 'healthy', 'timestamp': datetime.now().isoformat()}

@app.get('/models')
async def list_models():
    return {
        'models': {
            model_type: {
                'name': config['name'],
                'endpoint': config['endpoint'],
                'cost_per_token': config['cost_per_token']
            }
            for model_type, config in MODELS.items()
        }
    }

@app.post('/generate')
async def generate(request: ModelRequest):
    result = await generate_with_model(
        request.model_type,
        request.prompt,
        request.max_tokens,
        request.temperature,
        request.context
    )
    return result

# Specific model endpoints
@app.post('/ceo')
async def generate_ceo(request: ModelRequest):
    request.model_type = 'ceo'
    return await generate(request)

@app.post('/cto')
async def generate_cto(request: ModelRequest):
    request.model_type = 'cto'
    return await generate(request)

@app.post('/designer')
async def generate_designer(request: ModelRequest):
    request.model_type = 'designer'
    return await generate(request)

@app.post('/cfo')
async def generate_cfo(request: ModelRequest):
    request.model_type = 'cfo'
    return await generate(request)

@app.post('/healthcare')
async def generate_healthcare(request: ModelRequest):
    request.model_type = 'healthcare'
    return await generate(request)

@app.post('/multimedia')
async def generate_multimedia(request: ModelRequest):
    request.model_type = 'multimedia'
    return await generate(request)

@app.post('/global')
async def generate_global(request: ModelRequest):
    request.model_type = 'global'
    return await generate(request)

@app.post('/micro')
async def generate_micro(request: ModelRequest):
    request.model_type = 'micro'
    return await generate(request)

@app.post('/memory')
async def generate_memory(request: ModelRequest):
    request.model_type = 'memory'
    return await generate(request)

@app.post('/maestro')
async def generate_maestro(request: ModelRequest):
    request.model_type = 'maestro'
    return await generate(request)

if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host='0.0.0.0', port=8080)
        "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: iza-os-prometheus
    platform: linux/arm64
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: iza-os-grafana
    platform: linux/arm64
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=iza-os-admin-2024
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - ./monitoring/grafana:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped

  # Redis for Caching
  redis:
    image: redis:7-alpine
    container_name: iza-os-redis
    platform: linux/arm64
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # PostgreSQL for Metadata
  postgres:
    image: postgres:15-alpine
    container_name: iza-os-postgres
    platform: linux/arm64
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=iza_os_models
      - POSTGRES_USER=iza_os
      - POSTGRES_PASSWORD=iza-os-secure-2024
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U iza_os"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  default:
    name: iza-os-model-stack-inference-api
    driver: bridge

volumes:
  logs:
    driver: local
  data:
    driver: local
  monitoring:
    driver: local
EOF

# Create Prometheus configuration
echo "ðŸ“Š Creating Prometheus configuration..."
cat > $MONITORING_DIR/prometheus.yml << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'iza-model-gateway'
    static_configs:
      - targets: ['iza-model-gateway:8080']
    metrics_path: '/metrics'
EOF

# Create Grafana provisioning
echo "ðŸ“ˆ Creating Grafana provisioning..."
mkdir -p $MONITORING_DIR/grafana/provisioning/{datasources,dashboards}

cat > $MONITORING_DIR/grafana/provisioning/datasources/prometheus.yml << 'EOF'
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
EOF

# Start services
echo "ðŸš€ Starting IZA OS Hugging Face Model Stack (Inference API Version)..."
docker-compose -f docker-compose-inference-api.yml up -d

# Wait for services to be ready
echo "â³ Waiting for services to be ready..."
sleep 30

# Check service health
echo "ðŸ” Checking service health..."
docker-compose -f docker-compose-inference-api.yml ps

# Test endpoints
echo "ðŸ§ª Testing endpoints..."
curl -f http://localhost:8080/health || echo "âŒ Health check failed"

# Display access information
echo ""
echo "ðŸŽ‰ IZA OS Hugging Face Model Stack Deployed Successfully!"
echo "========================================================="
echo ""
echo "ðŸŒ Inference API Mode:"
echo "  â€¢ Using Hugging Face Inference API"
echo "  â€¢ No local model downloads required"
echo "  â€¢ Instant deployment"
echo "  â€¢ Pay-per-use pricing"
echo ""
echo "ðŸ“Š Access Points:"
echo "  â€¢ API Gateway:        http://localhost:8080/"
echo "  â€¢ CEO Agent:          http://localhost:8080/ceo"
echo "  â€¢ CTO Agent:          http://localhost:8080/cto"
echo "  â€¢ Designer Agent:     http://localhost:8080/designer"
echo "  â€¢ CFO Agent:          http://localhost:8080/cfo"
echo "  â€¢ Healthcare Agent:   http://localhost:8080/healthcare"
echo "  â€¢ Multi-Modal:       http://localhost:8080/multimedia"
echo "  â€¢ Multi-Lingual:     http://localhost:8080/global"
echo "  â€¢ Micro Agent:       http://localhost:8080/micro"
echo "  â€¢ Memory Agent:      http://localhost:8080/memory"
echo "  â€¢ Maestro Agent:     http://localhost:8080/maestro"
echo ""
echo "ðŸ“ˆ Monitoring:"
echo "  â€¢ Grafana Dashboard: http://localhost:3000 (admin/iza-os-admin-2024)"
echo "  â€¢ Prometheus:        http://localhost:9090"
echo ""
echo "ðŸ”§ Management:"
echo "  â€¢ View logs:         docker-compose -f docker-compose-inference-api.yml logs"
echo "  â€¢ Stop services:     docker-compose -f docker-compose-inference-api.yml down"
echo "  â€¢ Restart services:  docker-compose -f docker-compose-inference-api.yml restart"
echo ""
echo "ðŸ’° Cost Analysis (Inference API):"
echo "  â€¢ Hardware Investment: $0 (using existing M4 Max)"
echo "  â€¢ Operating Cost: Pay-per-use via HF API"
echo "  â€¢ No model storage required"
echo "  â€¢ Instant scaling"
echo ""
echo "ðŸŽ¯ Business Impact:"
echo "  â€¢ Ecosystem Value: $1.4B+"
echo "  â€¢ Revenue Pipeline: $10M+"
echo "  â€¢ Agent Count: 1,842"
echo "  â€¢ Automation Level: 95%"
echo "  â€¢ Team Efficiency: 98.7%"
echo ""
echo "âœ… Inference API Deployment Complete! Your autonomous venture studio is ready to scale."
echo ""
echo "ðŸ§ª Test your deployment:"
echo "curl -X POST http://localhost:8080/ceo \\"
echo "  -H 'Content-Type: application/json' \\"
echo "  -d '{\"prompt\": \"Create strategic plan for IZA OS expansion\", \"max_tokens\": 500}'"
